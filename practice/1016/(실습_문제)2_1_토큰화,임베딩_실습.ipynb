{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Migong0311/TIL/blob/main/practice/1016/(%EC%8B%A4%EC%8A%B5_%EB%AC%B8%EC%A0%9C)2_1_%ED%86%A0%ED%81%B0%ED%99%94%2C%EC%9E%84%EB%B2%A0%EB%94%A9_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4b6a34",
      "metadata": {
        "id": "1f4b6a34"
      },
      "source": [
        "\n",
        "# 🧠 토크나이저 / 워드 임베딩\n",
        "\n",
        "## 1.1 Tokenizer 학습\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 문장을 단어(토큰) 단위로 분리하여, 모델이 이해할 수 있는 숫자 시퀀스로 변환하는 과정입니다.\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “토크나이저는 어떤 기준으로 문장을 나누나요?”\n",
        "* “BPE(Byte Pair Encoding) 방식과 WordPiece의 차이는 무엇인가요?”\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 토크나이저를 이용한 토큰 ID 시퀀스 변환\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 학습된 토크나이저로 문장을 숫자 ID 시퀀스로 변환 (`encode`)\n",
        "* 다시 숫자를 문장으로 복원 (`decode`)\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “`encode()`와 `decode()`는 각각 어떤 역할을 하나요?”\n",
        "* “특수 토큰([CLS], [SEP], [PAD])은 어떤 상황에서 사용되나요?”\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 임베딩 벡터\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 토큰 ID를 고정 길이의 **연속 벡터**로 바꿔주는 층 (e.g., `nn.Embedding`)\n",
        "* 단어 간 의미적 유사도를 수치적으로 표현할 수 있게 함.\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “임베딩 벡터는 왜 필요한가요?”\n",
        "* “원-핫 인코딩과 임베딩의 차이는 무엇인가요?”\n",
        "* “`nn.Embedding`의 `vocab_size`와 `embedding_dim`은 각각 무엇을 의미하나요?”\n",
        "\n",
        "---\n",
        "\n",
        "# 🔁 RNN / LSTM\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 순차적 데이터를 처리하기 위한 대표적인 순환 신경망 구조\n",
        "* LSTM은 RNN의 장기 의존성 문제(vanishing gradient)를 해결함.\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “RNN이 시계열 데이터를 잘 다루는 이유는 뭔가요?”\n",
        "* “LSTM의 ‘cell state’는 어떤 역할을 하나요?”\n",
        "* “`nn.RNN`과 `nn.LSTM`의 입력·출력 형태 차이를 설명해주세요.”\n",
        "\n",
        "---\n",
        "\n",
        "# 🎯 Attention Mechanism\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 입력 시퀀스 전체를 요약하지 않고, 각 시점별 **가중치(attention weight)** 를 계산해 중요한 정보에 집중하는 메커니즘.\n",
        "* `LuongAttention`, `BahdanauAttention` 등이 대표적.\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “Attention의 핵심 아이디어는 무엇인가요?”\n",
        "* “Luong 어텐션과 Bahdanau 어텐션의 차이점은?”\n",
        "* “어텐션 가중치는 어떻게 계산되나요?”\n",
        "\n",
        "---\n",
        "\n",
        "# 🤗 HuggingFace 라이브러리 활용\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* 사전 학습된 모델(`BERT`, `GPT`, `T5`, `RoBERTa` 등)을 간편하게 불러와 사용할 수 있는 라이브러리.\n",
        "* Tokenizer, Model, Trainer 등 고수준 API 제공.\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “`from_pretrained()` 메서드는 어떤 역할을 하나요?”\n",
        "* “HuggingFace의 `pipeline`을 이용하면 어떤 일을 자동화할 수 있나요?”\n",
        "* “`tokenizer`, `model`, `trainer`는 각각 어떤 역할을 하나요?”\n",
        "\n",
        "---\n",
        "\n",
        "# 🧩 아키텍처별 모델 다뤄보기 (Encoder / Decoder)\n",
        "\n",
        "**핵심 개념**\n",
        "\n",
        "* **Encoder 모델**: 입력을 인코딩(요약)하는 역할 (e.g., BERT)\n",
        "* **Decoder 모델**: 입력을 바탕으로 출력 시퀀스를 생성 (e.g., GPT, T5)\n",
        "* **Seq2Seq 구조**: Encoder와 Decoder를 연결해 문장 변환/번역 등에 활용\n",
        "\n",
        "**질문 예시**\n",
        "\n",
        "* “Encoder와 Decoder의 구조적 차이는 무엇인가요?”\n",
        "* “BERT와 GPT는 각각 어떤 방식으로 텍스트를 처리하나요?”\n",
        "* “Seq2Seq 구조에서 Attention이 어디에 들어가나요?”\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add5dacf",
      "metadata": {
        "id": "add5dacf"
      },
      "source": [
        "### **Objectives**\n",
        "\n",
        "1. 실습명: 토큰화/임베딩 실습\n",
        "2. 핵심 주제\n",
        "    1) tokenizer를 이용하여 단어들을 토큰으로 변환하는 과정을 이해\n",
        "    2) 토큰화된 토큰들을 임베딩 벡터로 변환하는 과정을 이해\n",
        "    3) RNN부터 트랜스포머까지 모델의 발전사를 직접 체험하고 각 요소 기술의 역할을 이해\n",
        "3. 학습 목표\n",
        "    1) 토크나이저가 무엇이고 토큰화가 무엇인지에 대해서 설명할 수 있다.\n",
        "    2) 토큰화를 왜 하는지에 대해서 설명할 수 있다.\n",
        "    3) 토큰화된 토큰들을 임베딩 벡터로 변환하는 과정을 이해할 수 있다.\n",
        "    4) 임베딩 벡터를 이용하여 어떤 식으로 활용할 수 있는지 설명할 수 있다.\n",
        "    5) 다양한 모델의 발전사에 대해 직접 체험하고 각 아키텍쳐가 가지는 특징을 설명할 수 있다.\n",
        "\n",
        "4. 학습 개념\n",
        "    1) 토큰화:\n",
        "    2) 임베딩 벡터:\n",
        "    3) 인코더/디코더:\n",
        "  \n",
        "5. 학습 방향\n",
        "    - 실습은 아래 내용들을 직접 체험하고 각 아키텍쳐가 가지는 특징을 이해하는 것이 목표입니다.\n",
        "      - 토큰화\n",
        "      - 임베딩\n",
        "      - RNN\n",
        "      - LSTM\n",
        "      - 어텐션 메커니즘\n",
        "      - 인코더\n",
        "      - 디코더\n",
        "    - 실습 코드는 조교가 직접 구현한 코드를 참고하며 학습합니다.\n",
        "    - 자연스럽게 코드를 구현하면서 아키텍쳐의 발전사를 체험합니다.\n",
        "\n",
        "6. 데이터셋 개요 및 저작권 정보\n",
        "    - 데이터셋 명 : NSMC(Naver Sentiment Movie Corpus)\n",
        "    - 데이터셋 개요 : 네이버 영화 감정분석 데이터셋\n",
        "    - 데이터셋 저작권 : CC0 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26eeb4b",
      "metadata": {
        "id": "c26eeb4b"
      },
      "source": [
        "### **Prerequisites**\n",
        "```\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "tokenizers==0.21.4\n",
        "transformers==4.55.2\n",
        "torch==2.8.0+cu126\n",
        "```\n",
        "\n",
        "- 만약, 기본 코랩과 버전이 다르다면 아래 명령어를 복사해서 실행시켜주세요.\n",
        "```\n",
        "%pip install numpy==2.0.2 pandas==2.2.2 tokenizers==0.21.4 transformers==4.55.2 torch==2.8.0+cu126 --index-url https://download.pytorch.org/whl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "b1260a59",
      "metadata": {
        "id": "b1260a59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import (\n",
        "    Generic,\n",
        "    Tuple,\n",
        "    TypeVar,\n",
        "    List,\n",
        "    Union,\n",
        "    get_args\n",
        ")\n",
        "# 시드 설정\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "Batch = TypeVar(\"Batch\", bound=int)\n",
        "Token = TypeVar(\"Token\", bound=int)\n",
        "Sequence = TypeVar(\"Sequence\", bound=int)\n",
        "Layers = TypeVar(\"Layers\", bound=int)\n",
        "HiddenStates = TypeVar(\"HiddenStates\", bound=int)\n",
        "VocabSize = TypeVar(\"VocabSize\", bound=int)\n",
        "EmbeddingSize = TypeVar(\"EmbeddingSize\", bound=int)\n",
        "MaxLength = TypeVar(\"MaxLength\", bound=int)\n",
        "\n",
        "_1D = TypeVar(\"_1D\")\n",
        "_2D = TypeVar(\"_2D\")\n",
        "_3D = TypeVar(\"_3D\")\n",
        "\n",
        "def _label_str(self) -> str:\n",
        "    \"\"\"인스턴스의 제네릭 라벨 이름을 예쁘게 표시 (e.g., [Sequence])\"\"\"\n",
        "    oc = getattr(self, \"__orig_class__\", None)\n",
        "    if oc is None:\n",
        "        return \"[]\"\n",
        "    args = get_args(oc)\n",
        "    names = [getattr(a, \"__name__\", str(a)) for a in args]\n",
        "    return \"[\" + \", \".join(names) + \"]\"\n",
        "\n",
        "\n",
        "class Tensor1D(Generic[_1D]):\n",
        "    def __init__(self, tensor: torch.Tensor):\n",
        "        assert tensor.dim() == 1, ValueError(\"Tensor must be 1-dimensional\")\n",
        "        self.tensor = tensor\n",
        "        self.s: _1D = tensor.size(0)  # sequence length\n",
        "\n",
        "    def size(self) -> Tuple[int, int]:\n",
        "        return self.tensor.size()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor(shape=({self.s}))\"\n",
        "\n",
        "class Tensor2D(Generic[_1D, _2D]):\n",
        "    def __init__(self, tensor: torch.Tensor):\n",
        "        assert tensor.dim() == 2, ValueError(\"Tensor must be 2-dimensional\")\n",
        "        self.tensor = tensor\n",
        "        self.b: _1D = tensor.size(0)  # batch size\n",
        "        self.s: _2D = tensor.size(1)  # sequence length\n",
        "        assert self.b == tensor.size(0), ValueError(\n",
        "            f\"Expected batch {self.b}, but got {tensor.size(0)}\"\n",
        "        )\n",
        "        assert self.s == tensor.size(1), ValueError(\n",
        "            f\"Expected Sequence {self.s}, but got {tensor.size(1)}\"\n",
        "        )\n",
        "\n",
        "    def size(self) -> Tuple[int, int]:\n",
        "        return self.tensor.size()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor(shape=({self.b}, {self.s}))\"\n",
        "\n",
        "\n",
        "class Tensor3D(Generic[_1D, _2D, _3D]):\n",
        "    def __init__(self, tensor: torch.Tensor):\n",
        "        assert tensor.dim() == 3, ValueError(\"Tensor must be 3-dimensional\")\n",
        "        self.tensor = tensor\n",
        "        self.b: _1D = tensor.size(0)  # batch size\n",
        "        self.s: _2D = tensor.size(1)  # sequence length\n",
        "        self.h: _3D = tensor.size(2)  # hidden state size\n",
        "        assert self.b == tensor.size(0), ValueError(\n",
        "            f\"Expected batch {self.b}, but got {tensor.size(0)}\"\n",
        "        )\n",
        "        assert self.s == tensor.size(1), ValueError(\n",
        "            f\"Expected Sequence {self.s}, but got {tensor.size(1)}\"\n",
        "        )\n",
        "        assert self.h == tensor.size(2), ValueError(\n",
        "            f\"Expected Hidden State {self.h}, but got {tensor.size(2)}\"\n",
        "        )\n",
        "\n",
        "    def size(self) -> Tuple[int, int]:\n",
        "        return self.tensor.size()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor(shape=({self.b}, {self.s}, {self.h}))\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55eb901",
      "metadata": {
        "id": "d55eb901"
      },
      "source": [
        "# 1. 토크나이저 / 워드 임베딩\n",
        "\n",
        "- 학습 목표\n",
        "  1. 토크나이저를 학습할 수 있다.\n",
        "  2. 토크나이저를 사용하여 텍스트를 토큰 ID 시퀀스로 변환하는 방법을 이해하고 구현할 수 있ㅏ.\n",
        "- 학습 개념\n",
        "  1. 토크나이저\n",
        "  2. 토큰화\n",
        "  3. 임베딩\n",
        "- 진행하는 실습 요약\n",
        "  1. 제공된 말뭉치로 WordPiece 토크나이저를 훈련시키는 코드 한 줄을 완성\n",
        "  2. 훈련된 토크나이저를 사용해 특정 문장을 토큰 ID 시퀀스로 변환하는 코드\n",
        "  3. nn.Embedding 레이어(혹은 간단한 dict lookup)를 사용하여 주어진 토큰 ID에 해당하는 임베딩 벡터를 조회하는 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42fdfec",
      "metadata": {
        "id": "c42fdfec"
      },
      "source": [
        "### 1.1. Tokenizer 학습\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 토크나이저 학습</b><br>\n",
        "언어 모델에서 토크나이저는 텍스트를 토큰으로 변환하는 역할을 합니다. 토크나이저를 학습하는 방법에 대해 알아봅니다.\n",
        "</blockquote>\n",
        "\n",
        "토크나이저를 학습하기 위해서는 다음 두가지가 필요합니다.\n",
        "1. 토크나이저 객체(클래스)\n",
        "2. 학습 데이터\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc62bac",
      "metadata": {
        "id": "9cc62bac"
      },
      "source": [
        "그러면 우선 학습 데이터를 준비해보겠습니다.\n",
        "\n",
        "학습할 텍스트 데이터가 들어있는 파일을 준비합니다.\n",
        "\n",
        "여기서는 NSMC(Naver Sentiment Movie Corpus) 데이터셋을 사용하겠습니다.\n",
        "\n",
        "아래 명령어를 실행하여 데이터셋을 다운로드 받습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "97b7effd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97b7effd",
        "outputId": "5350b2e7-2dca-4caa-a4b9-4d2778cf3fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 07:56:18--  https://github.com/e9t/nsmc/raw/master/ratings.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt [following]\n",
            "--2025-10-17 07:56:19--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19515078 (19M) [text/plain]\n",
            "Saving to: ‘ratings.txt.4’\n",
            "\n",
            "ratings.txt.4       100%[===================>]  18.61M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-17 07:56:19 (125 MB/s) - ‘ratings.txt.4’ saved [19515078/19515078]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/e9t/nsmc/raw/master/ratings.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "facaabde",
      "metadata": {
        "id": "facaabde"
      },
      "source": [
        "데이터셋을 확인해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "dd94219a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "dd94219a",
        "outputId": "836cba2d-76e1-482a-9785-3d6b93935b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습에 필요한 파일이 존재합니다! ratings.txt\n",
            "리뷰 갯수 : 199992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a9e55e0-71da-4716-a250-5cb16fc3dddc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a9e55e0-71da-4716-a250-5cb16fc3dddc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a9e55e0-71da-4716-a250-5cb16fc3dddc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a9e55e0-71da-4716-a250-5cb16fc3dddc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-059aac59-16b6-46cb-86b6-5b2f01a41849\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-059aac59-16b6-46cb-86b6-5b2f01a41849')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-059aac59-16b6-46cb-86b6-5b2f01a41849 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "file_list = os.listdir()\n",
        "for file in file_list:\n",
        "    if \"ratings.txt\" == file:\n",
        "        print('학습에 필요한 파일이 존재합니다!', file)\n",
        "        df = pd.read_table( (os.getcwd() + '/' + file), encoding='utf-8') # 데이터 프레임으로 보기 편하게 바꿔줍시다!\n",
        "        df = df.dropna(how = 'any') # 널값을 없애줍니다!\n",
        "        print('리뷰 갯수 :', len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3146be",
      "metadata": {
        "id": "ed3146be"
      },
      "source": [
        "텍스트 데이터가 있는 'document'열만을 가져오고\n",
        "\n",
        "해당 데이터를 txt 파일로 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "c0224daf",
      "metadata": {
        "id": "c0224daf"
      },
      "outputs": [],
      "source": [
        "with open((os.getcwd() + '/' + 'naver_review.txt'), 'w', encoding='utf8') as f:\n",
        "    # TODO: document 열만 가져와서 저장하는 코드를 구현합니다.\n",
        "    # FIXME\n",
        "\n",
        "     for line in df['document']:\n",
        "        # None, NaN 같은 결측치 방지\n",
        "        if pd.notnull(line):\n",
        "            f.write(str(line).strip() + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252458b8",
      "metadata": {
        "id": "252458b8"
      },
      "source": [
        "학습이 되어 있지 않은 빈 tokenizer를 생성합니다.\n",
        "\n",
        "여기서는 BertWordPieceTokenizer를 불러옵니다.\n",
        "\n",
        "##### 파라미터:\n",
        "- `strip_accents` : 입력 텍스트의 악센트(액센트)를 제거할지 여부를 결정하는 옵션입니다. 한국어를 학습할때에는 `False`로 설정합니다.\n",
        "- `lowercase` : 영어를 모두 소문자로 바꿉니다. `False`로 설정하면 영어를 대문자로 유지합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "9d919be5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d919be5",
        "outputId": "9e4473a2-02a5-4f2a-a2bd-822b2548c5ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=0, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=##)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# 빈 tokenizer 생성 : vocabulary_size = 0 인 것을 확인하실 수 있습니다.\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    lowercase=False,\n",
        "    strip_accents=False,\n",
        ")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4e500b",
      "metadata": {
        "id": "3a4e500b"
      },
      "source": [
        "아래 코드를 실행하여 토크나이저를 학습합니다.\n",
        "#### 파라미터 설명:\n",
        "- `data_file` : 데이터 경로를 지정해줍니다. list 형태로 여러개의 파일을 지정해줄수도 있습니다.\n",
        "- `vocab_size (default: 30000)` : 단어사전 크기를 지정할 수 있습니다. 어떠한 값이 가장 좋다는 것은 없지만, 값이 클수록 많은 단어의 의미를 담을 수 있습니다.\n",
        "- `initial_alphabet` : 꼭 포함됐으면 하는 initial alphabet을 학습 전에 추가해줍니다.\n",
        "    - initial은 학습하기 이전에 미리 단어를 vocab에 넣는 것을 의미합니다.\n",
        "    - special token들도 initial에 vocab에 추가됩니다.\n",
        "- `limit_alphabet (default: 1000)` : initial tokens의 갯수를 제한합니다.\n",
        "- `min_frequency (default: 2)` : 최소 빈도수를 의미합니다. 만약 어떤 단어가 1번 나오면 vocab에 추가하지 않습니다.\n",
        "- `special_tokens` : 특수 토큰을 넣을 수 있습니다.. BERT에는 다음과 같은 토큰이 들어가야 합니다.\n",
        "    - `[PAD]` : 패딩을 위한 토큰\n",
        "    - `[UNK]` : OOV 단어를 위한 토큰\n",
        "    - `[CLS]` : 문장의 시작을 알리고 분류 문제에 사용되는 토큰\n",
        "    - `[SEP]` : 문장 사이사이를 구별해주는 토큰\n",
        "    - `[MASK]` : MLM 태스크를 위한 마스크 토큰\n",
        "- `wordpiece_prefix(default: '##')` : sub-word라는 것을 알려주는 표시입니다.\n",
        "    - BERT는 기본적으로 '##'을 씁니다.\n",
        "    - 예를 들어, `SS, ##AF, ##Y` 처럼 sub-word를 구분하기 위해 '##'을 사용합니다.\n",
        "- `show_progress` : 학습 과정을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "82aa578c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82aa578c",
        "outputId": "9a914edc-7143-489c-bc53-f059c54ebb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size :  30000\n",
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/']\n"
          ]
        }
      ],
      "source": [
        "data_file = 'naver_review.txt'\n",
        "vocab_size = 30000\n",
        "min_frequency = 2\n",
        "initial_alphabet = []\n",
        "limit_alphabet = 6000\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "wordpieces_prefix = \"##\"\n",
        "show_progress=True\n",
        "\n",
        "tokenizer.train(\n",
        "    files = data_file,\n",
        "    vocab_size = vocab_size,\n",
        "    min_frequency = min_frequency,\n",
        "    initial_alphabet = initial_alphabet,\n",
        "    limit_alphabet = limit_alphabet,\n",
        "    special_tokens = special_tokens,\n",
        "    wordpieces_prefix = wordpieces_prefix,\n",
        "    show_progress = True,\n",
        ")\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(\"vocab size : \", len(vocab))\n",
        "print(sorted(vocab, key=lambda x: vocab[x])[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83244f41",
      "metadata": {
        "id": "83244f41"
      },
      "source": [
        "### 1.2. 토크나이저를 이용한 토큰 ID 시퀀스 반환\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 토크나이저를 이용한 토큰 ID 시퀀스 반환</b><br>\n",
        "모델이 토큰을 이해하기 위해서는 정수값으로 반환하는 과정이 필요합니다. 토크나이저를 이용하여 텍스트 토큰을 ID 시퀀스로 변환합니다.\n",
        "</blockquote>\n",
        "\n",
        "아래 코드를 실행하여 토크나이저를 이용하여 텍스트 토큰을 ID 시퀀스로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "a26d8b6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26d8b6b",
        "outputId": "ad504d24-78ab-40d2-bab8-6d976661a00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌱토큰화 결과 : ['I', \"'\", 'm', 'a', 'st', '##ud', '##ent', 'of', 'S', '##S', '##A', '##F', '##Y', '!']\n",
            "🌱정수 인코딩 : [45, 11, 81, 69, 15444, 24900, 16071, 10280, 55, 3890, 4047, 3976, 4755, 5]\n",
            "🌈디코딩 : I ' m a student of SSAFY!\n"
          ]
        }
      ],
      "source": [
        "text = \"I'm a student of SSAFY!\"\n",
        "\n",
        "encoded = tokenizer.encode(text)\n",
        "print('🌱토큰화 결과 :',encoded.tokens)\n",
        "print('🌱정수 인코딩 :',encoded.ids)\n",
        "print('🌈디코딩 :',tokenizer.decode(encoded.ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884cec1b",
      "metadata": {
        "id": "884cec1b"
      },
      "source": [
        "<blockquote>\n",
        "<b>🧠 토크나이저를 이용한 모델 입력 만들기</b><br>\n",
        "그렇다면 모델의 입력으로 넣기 위해서는 어떤 방식으로 토크나이징을 해야 할까요?\n",
        "</blockquote>\n",
        "\n",
        "위에 대한 답변은 앞으로 실습 코드를 진행하면서 나오기 때문에 이 점을 잊지 말고 계속 따라가시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a906dbd",
      "metadata": {
        "id": "7a906dbd"
      },
      "source": [
        "### 1.3. 임베딩 벡터\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 토큰 ID에 따라 어떤 방식으로 벡터화가 될까요?</b><br>\n",
        "토큰 ID에 해당하는 임베딩 벡터를 확인해보겠습니다.\n",
        "</blockquote>\n",
        "\n",
        "아래 코드를 실행하여 특정 토큰 ID에 따른 임베딩 벡터를 확인해보겠습니다.\n",
        "\n",
        "임베딩 벡터는 torch의 nn.Embedding 모듈을 사용하여 생성됩니다. 해당 임베딩 벡터는 모두 임의의 값으로 초기화됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "db1e9b06",
      "metadata": {
        "id": "db1e9b06"
      },
      "outputs": [],
      "source": [
        "# embedding_vector = nn.Embedding()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e6473c",
      "metadata": {
        "id": "e7e6473c"
      },
      "source": [
        "임베딩 벡터를 초기화하려고 하니 다음 두가지 파라미터를 반드시 넣으라고 합니다.\n",
        "\n",
        "1. `num_embeddings`: 임베딩 사전의 크기 (size of the dictionary of embeddings)\n",
        "2. `embedding_dim`: 각 임베딩 벡터의 차원 (the size of each embedding vector)\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 num_embeddings </b><br>\n",
        "임베딩 사전의 크기는 무슨 의미일까요?\n",
        "</blockquote>\n",
        "\n",
        "여기서 `num_embeddings`는 고유한 토큰(단어, 문자 등)의 총 개수를 의미합니다. 즉, 어떤 `인덱스 → 벡터` 매핑 테이블을 만들 건데, 그 테이블에 몇 개의 항목이 들어가야 하는지를 정의하는 값입니다. tokenizer를 만들때 `vocab_size`와 동일한 값을 의미합니다.\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 embedding_dim </b><br>\n",
        "각 임베딩 벡터의 차원은 무슨 의미일까요?\n",
        "</blockquote>\n",
        "\n",
        "`embedding_dim`은 각 단어(또는 토큰)가 표현되는 벡터의 길이입니다. 즉, 하나의 단어를 어떤 숫자 벡터로 나타낼 때 그 벡터가 몇 차원인지 정하는 값입니다. 보통의 embedding은 `768`, `1024` 등 2의 제곱수 차원을 사용합니다. (\"어떤 값이 정답이다\" 하는 값이 있는 건 아닙니다.)\n",
        "\n",
        "여기서는 vocab_size와 embedding_dim을 768로 정의해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "d9723eee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9723eee",
        "outputId": "7e28bd65-d81f-49fa-8278-533f003c8e13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "embedding_vector: Tensor2D[VocabSize, EmbeddingSize] = nn.Embedding(vocab_size, 768)\n",
        "embedding_vector.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a9214e",
      "metadata": {
        "id": "98a9214e"
      },
      "source": [
        "그러면 특정 토큰의 임베딩 벡터를 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "8b4c3059",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4c3059",
        "outputId": "ee23f9b0-8753-4344-ece4-dd8fa2b4fad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_id: 45\n",
            "input_id 차원: torch.Size([1])\n",
            "vector 차원: torch.Size([1, 768])\n",
            "vector: tensor([[-5.8641e-01, -1.1327e+00,  2.6612e-02, -3.6936e-01, -4.5574e-01,\n",
            "          1.4395e+00, -2.7049e-01, -2.3921e-02,  4.3165e-01,  6.3602e-01,\n",
            "         -4.0117e-01, -1.0804e+00, -6.4650e-01, -6.8504e-02,  2.4397e-01,\n",
            "         -2.0591e-01, -1.8770e-01,  4.2026e-01,  7.1682e-01, -5.9828e-01,\n",
            "          3.1360e-01,  1.8200e+00,  2.8490e+00,  1.3980e+00,  1.0531e+00,\n",
            "          2.0170e+00,  6.0673e-01, -1.5876e+00,  1.1668e+00, -3.1769e-01,\n",
            "         -5.3360e-01, -4.7004e-01, -9.2409e-01,  1.3773e+00, -1.3743e-01,\n",
            "          4.2839e-02, -4.8446e-01, -9.6651e-01, -1.5018e+00, -4.8411e-01,\n",
            "          1.3622e+00, -1.7072e+00, -7.3317e-01,  2.9438e-01, -1.0314e+00,\n",
            "          1.7281e+00,  1.4170e+00,  1.2014e-01, -1.5709e+00, -3.1901e-01,\n",
            "         -2.0575e-02,  6.4082e-02, -1.9547e-01, -4.9615e-01,  4.1448e-01,\n",
            "         -2.1306e-01, -8.5294e-02,  5.7862e-01,  9.8439e-02,  7.3975e-01,\n",
            "          2.4581e-02,  9.2886e-02, -3.3140e-01, -1.3073e-01,  1.6888e+00,\n",
            "         -1.6246e-01,  3.6534e-01,  1.8052e+00, -5.4247e-01,  1.1382e+00,\n",
            "         -1.1691e-02, -6.5054e-01, -2.9788e+00, -1.7490e+00, -5.2741e-01,\n",
            "         -8.1005e-01,  3.9198e-01,  1.4487e-01,  4.4438e-01, -9.0061e-01,\n",
            "          2.6250e-01, -2.9051e-01,  2.2310e-01, -1.2519e+00,  1.2721e+00,\n",
            "          6.2651e-01,  1.5596e+00, -8.3331e-02, -4.1976e-01,  1.7190e+00,\n",
            "         -8.3126e-01,  1.6253e+00, -2.3224e-01,  8.6379e-01,  6.8156e-01,\n",
            "         -1.7146e+00, -1.4344e-01,  5.6220e-02, -1.8790e+00, -9.5746e-01,\n",
            "          6.4773e-01,  1.7119e-01, -5.1799e-01, -2.9002e-01, -1.4499e+00,\n",
            "         -2.2011e+00, -1.9358e+00,  1.7292e+00,  3.2779e-01,  9.1354e-01,\n",
            "         -1.8337e-01, -1.0150e+00,  6.1067e-02, -1.6252e-01, -1.8904e-01,\n",
            "         -3.1064e-01,  6.4264e-01,  2.4448e+00, -8.2296e-01,  2.3123e-01,\n",
            "          5.2089e-01,  2.2307e-01,  8.9294e-01, -3.4659e-01, -2.2180e+00,\n",
            "         -2.9681e-01,  1.9733e-01,  1.0184e+00, -5.8794e-01,  1.0451e+00,\n",
            "          1.6000e+00,  1.0269e-01, -6.3995e-01,  2.5967e-01, -1.2352e-01,\n",
            "          6.0335e-01, -1.0959e-01,  1.3712e+00,  2.6667e-01, -1.0211e+00,\n",
            "          2.7895e-01, -1.2074e+00, -9.0694e-01,  8.2587e-01,  2.1134e-01,\n",
            "          4.1955e-01, -2.6536e-01, -1.1998e+00,  5.6761e-01,  1.3631e+00,\n",
            "         -1.1470e+00,  2.3841e-01, -1.0672e+00,  1.2854e+00, -2.5034e+00,\n",
            "          1.0756e+00,  7.8051e-02, -1.1610e+00,  2.3808e+00,  5.8393e-03,\n",
            "          1.2712e+00,  6.9454e-01, -2.0363e+00,  4.2008e-01,  1.4959e+00,\n",
            "         -2.6621e+00, -1.1025e+00, -8.7692e-01,  2.7085e-01, -3.6835e-01,\n",
            "         -4.5597e-01,  1.6099e-01,  2.2385e+00,  7.7596e-01,  6.9936e-01,\n",
            "          1.2859e-02, -1.0188e+00,  2.0262e-01,  1.0505e-01, -5.3979e-01,\n",
            "         -1.1471e+00,  1.0325e+00,  2.2522e-01,  1.7243e-01,  8.4644e-01,\n",
            "         -3.9182e-01, -1.3194e+00,  1.9120e+00,  1.1990e+00, -7.1338e-01,\n",
            "         -2.9062e-01, -1.9692e+00, -1.2352e+00,  3.4469e-01,  3.7266e-01,\n",
            "          7.4043e-03,  1.7676e-01,  4.4155e-01, -2.8602e-01,  1.1501e+00,\n",
            "         -4.9747e-01, -2.1877e+00,  1.2597e+00, -4.6811e-01, -1.9778e+00,\n",
            "          9.2266e-02,  8.3967e-01, -1.2875e+00, -4.5443e-01,  8.2618e-01,\n",
            "          1.3657e+00, -1.6183e-01,  2.7951e+00,  5.9410e-01, -8.6167e-01,\n",
            "         -2.4812e+00,  5.2129e-01,  1.0565e+00,  8.2845e-01, -3.9922e-01,\n",
            "          1.6755e+00,  2.2296e+00,  1.4605e+00, -1.3790e+00,  8.9979e-02,\n",
            "         -8.4355e-01, -6.2137e-01,  8.8655e-01, -7.5264e-01,  4.5718e-02,\n",
            "         -8.8061e-01, -1.3311e+00,  2.6024e+00, -8.8568e-02,  3.6131e-01,\n",
            "         -9.6495e-01,  2.5996e-01, -4.8475e-01, -1.1931e+00, -3.5965e-03,\n",
            "          4.5652e-01,  1.1003e+00,  6.0825e-01,  5.0255e-01,  6.4847e-02,\n",
            "         -2.0972e-01, -8.1905e-01,  6.3713e-01,  5.5610e-01, -3.7969e-01,\n",
            "         -8.2476e-01,  3.0568e-01,  1.3790e+00,  1.0315e+00, -1.1049e+00,\n",
            "         -1.2539e+00,  7.8217e-01,  4.2802e-01,  5.2194e-01, -4.2326e-01,\n",
            "          6.9081e-01, -1.1530e+00,  1.0535e+00,  8.4309e-01, -1.5127e+00,\n",
            "          1.5953e+00,  1.5544e+00, -3.3269e-01,  1.8270e+00,  2.3450e-01,\n",
            "         -2.6720e-01, -4.8002e-01, -7.2525e-01, -6.9044e-02,  1.8810e-02,\n",
            "          1.4364e-01, -2.2373e+00,  4.0876e-01, -3.7807e-02, -8.5370e-01,\n",
            "         -1.4437e+00, -1.5806e+00,  1.6538e+00,  6.1369e-01,  5.1194e-01,\n",
            "         -1.4412e+00,  6.9254e-01,  1.6215e+00, -8.9781e-01, -2.6808e-01,\n",
            "          6.8007e-01, -2.5736e+00,  3.0228e-01, -2.1020e-01,  4.0738e-01,\n",
            "         -1.2795e+00, -2.1194e+00, -2.3627e-01,  6.4997e-01,  2.0307e-01,\n",
            "         -4.0661e-01, -9.2588e-01, -6.6186e-01, -9.7114e-01, -1.2413e+00,\n",
            "          1.6439e-01,  1.1636e+00,  3.6198e-01, -1.1532e+00, -1.0988e+00,\n",
            "          5.3209e-01, -8.9032e-01,  1.1218e+00,  5.7064e-01,  7.5646e-01,\n",
            "          1.3166e+00,  1.2176e+00, -5.0478e-01, -6.4268e-01,  2.3216e+00,\n",
            "          1.1027e+00,  2.3127e-01,  4.6957e-01, -6.8633e-01, -1.4759e+00,\n",
            "          1.4607e-01, -8.5485e-01,  5.1082e-01,  2.7225e-01, -1.0504e+00,\n",
            "         -1.8880e+00,  8.0767e-01, -1.0015e+00, -2.9965e-01, -6.4340e-01,\n",
            "         -1.1795e+00,  1.2514e+00, -1.1169e+00, -6.2890e-03, -2.4045e+00,\n",
            "         -1.5852e-01,  7.9345e-01, -2.9591e-01,  8.2604e-01, -3.5559e-01,\n",
            "          1.7587e-01,  1.5028e-01, -2.7314e-01,  3.9683e-02, -1.1727e+00,\n",
            "         -1.7857e+00, -1.6997e+00,  9.1090e-01,  6.2496e-01,  7.2536e-02,\n",
            "         -2.5993e-01, -7.4421e-01, -1.4108e+00,  1.1926e-01,  9.8203e-01,\n",
            "          5.4706e-01,  1.0072e-01, -7.9624e-02,  7.1058e-02,  1.7737e-01,\n",
            "          7.5613e-01, -1.2531e+00,  6.6835e-01, -1.0337e+00, -9.1633e-01,\n",
            "          4.8913e-01, -4.7103e-01, -4.2296e-01, -8.8159e-01, -1.6258e+00,\n",
            "          8.6559e-01,  8.9447e-01,  1.0847e+00,  2.9223e-01,  1.3801e+00,\n",
            "          5.5510e-01,  2.3797e-01,  1.6000e-01, -2.7714e-01, -1.3358e+00,\n",
            "         -1.5762e-01, -1.0363e+00,  9.8573e-01, -1.1911e+00,  2.4253e-01,\n",
            "          5.4839e-01,  1.3786e+00,  1.5118e+00,  2.5880e-01,  1.1013e+00,\n",
            "          1.4151e+00, -1.2057e-01,  1.3439e-01, -1.5628e+00, -6.1287e-01,\n",
            "         -1.1348e+00, -9.6102e-01,  1.7011e+00,  5.0617e-01, -1.9768e+00,\n",
            "         -5.6951e-01,  1.5390e+00,  6.3183e-02, -1.6705e+00,  1.4659e+00,\n",
            "          1.1269e+00, -3.0730e+00, -1.1650e+00,  1.1070e+00,  2.4070e-01,\n",
            "          2.7651e-01, -1.2080e+00,  4.8507e-01,  7.9149e-02,  5.4360e-01,\n",
            "         -8.3520e-01,  9.3864e-01, -8.5778e-01,  3.6294e-01,  1.0618e+00,\n",
            "          1.3808e-01, -3.9061e-02, -3.5348e-01,  2.1225e+00, -7.2674e-01,\n",
            "         -1.7458e+00,  1.0003e+00,  2.8215e-02,  1.1499e+00,  4.2818e-01,\n",
            "          1.5518e+00, -8.6611e-01,  1.4710e+00, -8.2705e-01,  9.8005e-02,\n",
            "          2.6920e+00,  1.3048e+00,  6.0844e-01, -1.2385e+00,  4.6886e-01,\n",
            "         -8.6682e-01,  1.0413e+00, -2.7178e-01,  1.4223e+00,  2.9257e+00,\n",
            "          9.2931e-02,  5.7789e-01, -1.5264e+00,  3.8429e-01,  3.2746e-01,\n",
            "         -1.1850e+00,  1.0487e+00, -1.0260e-01,  5.1305e-01, -4.3763e-01,\n",
            "         -1.1442e+00,  1.6211e-01,  1.0189e-01, -8.6003e-01,  3.7350e-01,\n",
            "         -2.0934e+00,  9.4786e-01,  8.0349e-01,  9.0876e-02, -7.2433e-01,\n",
            "          8.9107e-01,  2.8971e-01,  1.4685e+00, -9.0758e-01,  1.0190e+00,\n",
            "         -1.1583e+00, -1.5949e+00, -2.5171e-01,  2.0537e-01, -1.7279e+00,\n",
            "          2.3990e-01, -2.2937e+00,  6.8148e-01,  1.5945e+00,  4.0917e-01,\n",
            "         -7.5195e-01,  1.1046e-01, -1.5076e+00, -8.0238e-01,  2.2897e-01,\n",
            "          6.2368e-01, -3.0776e-01, -1.5446e-01,  9.0203e-01,  2.1589e+00,\n",
            "          1.4359e-02, -2.0427e-01,  4.5405e-01,  1.6242e+00,  2.4179e+00,\n",
            "          1.9296e-02, -1.0937e+00, -4.3272e-01, -8.0123e-01,  1.6898e-01,\n",
            "         -7.9126e-01, -3.3747e-01, -2.6263e-01,  6.3985e-01,  1.9795e+00,\n",
            "          5.0256e-01,  4.2781e-01,  9.2407e-01, -1.1289e+00,  1.6008e+00,\n",
            "          2.7487e-01,  2.4097e-01,  6.6713e-01, -5.3765e-01, -3.8973e-01,\n",
            "          5.5697e-01,  5.1861e-02, -1.0619e+00, -1.9407e+00, -1.1581e+00,\n",
            "          3.4482e-01, -7.0241e-01,  1.4376e+00, -1.1725e+00, -3.8754e-01,\n",
            "          6.0148e-01, -4.2554e-01, -1.6213e-01,  6.2803e-02,  5.8063e-01,\n",
            "         -3.4327e-01, -4.5692e-01,  9.6038e-01,  6.9764e-01,  1.0929e+00,\n",
            "          1.7693e+00, -3.1421e-02, -1.0884e+00, -1.0127e-01, -1.9810e-01,\n",
            "          4.4012e-02, -1.2764e+00, -1.0442e+00,  4.5751e-01,  3.7497e-01,\n",
            "         -8.9870e-01, -8.6049e-03,  1.6212e+00,  2.1729e-01, -1.3368e+00,\n",
            "         -4.6538e-04,  5.7354e-01, -1.4653e+00,  1.1711e+00,  2.9986e-01,\n",
            "          2.3952e-01, -6.4066e-01,  3.2257e-01, -1.0702e+00, -4.5255e-01,\n",
            "          8.4570e-01,  8.7082e-01, -1.2804e+00, -8.0807e-01, -1.0393e+00,\n",
            "          1.8883e-01,  2.9374e-01,  1.8074e-02,  1.1724e+00, -5.2455e-01,\n",
            "         -4.1127e-01,  6.3151e-01, -3.2293e-01,  1.1213e+00,  8.8539e-01,\n",
            "         -7.4528e-01,  2.5474e+00,  4.3508e-01,  1.5564e-01, -2.8813e-01,\n",
            "         -1.4950e+00,  1.9958e+00, -4.4929e-01,  5.7363e-01,  1.3688e+00,\n",
            "          9.8464e-01, -1.5622e+00,  2.9645e-01,  7.9629e-02, -1.0188e+00,\n",
            "         -2.8821e-01,  6.5786e-01, -9.4752e-01, -6.8457e-02,  5.4615e-01,\n",
            "          3.1922e-01, -9.3858e-01,  2.0790e+00,  2.0641e-01,  1.3692e+00,\n",
            "          3.6679e-01,  1.2206e-01,  3.4521e-01,  2.1476e-01,  9.4113e-01,\n",
            "          9.7986e-01, -1.2927e+00, -4.6423e-01, -2.4949e-01,  1.5714e+00,\n",
            "          6.6313e-01,  4.8623e-01,  7.1229e-02, -1.1668e-01,  1.0935e+00,\n",
            "         -4.1720e-01,  1.2234e-01,  2.5845e-01, -5.0578e-01,  1.6090e+00,\n",
            "          4.8093e-02,  2.7948e-01,  4.5103e-01, -5.2214e-01, -1.2607e+00,\n",
            "         -5.7107e-01, -5.7751e-01,  6.6488e-01, -1.7756e+00,  6.5838e-01,\n",
            "         -2.7618e-01, -1.5014e+00,  5.9242e-01,  1.7892e+00,  1.5430e+00,\n",
            "          2.9727e-01,  1.8970e+00,  2.3541e-01,  7.3836e-01, -2.1829e-01,\n",
            "         -5.6520e-01, -5.2170e-01, -2.8841e-01,  1.7133e+00,  1.3969e-01,\n",
            "         -4.4374e-01, -1.5180e+00,  5.8020e-01, -1.4620e-01, -1.8151e+00,\n",
            "          1.8509e+00, -5.7693e-01,  6.2626e-01, -7.9069e-01, -1.7662e+00,\n",
            "         -7.7322e-01, -1.0780e-01,  8.9673e-01,  2.0367e+00,  6.2815e-01,\n",
            "         -5.8681e-01, -1.0963e+00,  5.1416e-01, -1.6618e+00, -2.3677e-01,\n",
            "          5.0208e-01, -3.8207e-01,  4.7563e-01, -7.4501e-01, -1.4155e-01,\n",
            "         -2.1625e-01, -1.9711e+00, -1.2997e+00,  6.5497e-01, -2.1389e-01,\n",
            "         -1.5022e-01, -4.7207e-01,  1.0380e+00,  1.0139e+00, -1.5492e+00,\n",
            "         -9.2274e-01, -3.6818e-03, -1.9299e+00, -3.1747e-01,  4.3391e-01,\n",
            "          6.2427e-01,  9.4415e-01, -1.2236e+00, -1.2878e-01,  3.8714e-01,\n",
            "          9.4548e-01, -8.0918e-01, -2.0064e+00,  4.8596e-02, -9.9246e-01,\n",
            "          4.3346e-01,  1.9962e-01, -1.0281e+00,  1.2949e+00, -9.7883e-01,\n",
            "          7.2204e-01,  1.6217e-01,  1.1873e+00, -1.6302e+00, -1.6191e-01,\n",
            "          7.4264e-01,  1.2234e-01,  5.3141e-02, -1.7858e+00,  9.8436e-02,\n",
            "         -5.4184e-01,  4.1263e-01,  8.8509e-01, -3.8275e-01, -5.5372e-01,\n",
            "         -1.0093e+00, -1.8745e+00, -3.1291e-02, -6.5305e-01,  9.3340e-01,\n",
            "         -8.9972e-01,  1.0360e+00, -8.8613e-01, -3.8863e-01,  2.2449e-01,\n",
            "         -1.0740e+00,  5.8647e-01, -8.3409e-01, -2.4781e+00, -1.6360e+00,\n",
            "         -1.2667e+00,  1.0647e+00,  6.0862e-01,  1.0411e+00,  4.9001e-01,\n",
            "         -5.6568e-01, -7.7290e-02,  1.6889e+00, -5.3592e-01,  3.0126e-01,\n",
            "         -9.3709e-01, -1.0054e+00,  6.8849e-02,  6.9186e-01, -1.2959e+00,\n",
            "          8.7209e-01,  2.7381e-01,  1.6178e+00,  3.3304e-01,  2.3623e+00,\n",
            "          4.5610e-01, -7.9843e-02, -2.8436e-01, -1.5437e+00, -1.1984e+00,\n",
            "         -7.8466e-01,  1.1022e+00,  1.5757e-01, -1.2095e+00,  2.2163e-01,\n",
            "          6.4150e-01,  1.0414e+00,  8.6908e-01]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_id = tokenizer.token_to_id(\"I\")\n",
        "print(\"token_id:\", token_id)\n",
        "input_id = torch.tensor([token_id], dtype=torch.long)\n",
        "print(\"input_id 차원:\", input_id.shape)\n",
        "\n",
        "vector = embedding_vector(input_id)\n",
        "print(\"vector 차원:\", vector.shape)\n",
        "print(\"vector:\", vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f47d8d6",
      "metadata": {
        "id": "7f47d8d6"
      },
      "source": [
        "# 2. RNN/LSTM\n",
        "\n",
        "- 학습 목표\n",
        "  1. RNN/LSTM을 이용하여 문장 전체의 정보를 압축한 문맥 벡터에 대한 이해를 할 수 있다.\n",
        "  2. Encoder Decoder 구조를 통해 문맥 벡터를 이용하여 특정 task를 수행할 수 있다.\n",
        "- 학습 개념\n",
        "  1. RNN/LSTM\n",
        "  2. Encoder/Decoder\n",
        "- 진행하는 실습 요약\n",
        "  1. 간단한 RNN/LSTM을 구현한다.\n",
        "  2. 번역 task와 관련된 encoder decoder 구조를 구현한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6cfb8ca",
      "metadata": {
        "id": "d6cfb8ca"
      },
      "source": [
        "<blockquote>\n",
        "<b>🧠 Recurrent Neural Network(RNN)이란? </b><br>\n",
        "순차적(Sequential) 이전의 정보를 기억하여 현재의 정보를 처리하는 신경망 구조를 의미합니다.\n",
        "</blockquote>\n",
        "\n",
        "RNN이 갖는 특징은 다음과 같습니다.\n",
        "\n",
        "- 입력을 순차적으로 처리합니다.\n",
        "- RNN은 같은 가중치를 반복적으로 사용합니다.\n",
        "- 재귀적인 구조를 가집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4dfc5c",
      "metadata": {
        "id": "ec4dfc5c"
      },
      "source": [
        "그러면 이제부터 입력 텍스트를 RNN에 입력으로 넣어서 출력층의 결과값을 받아봅시다!\n",
        "\n",
        "텍스트를 입력으로 넣기 위해서는 위에서 보았듯, 워드 임베딩으로 변환해야 합니다.\n",
        "워드 임베딩을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "2b720ce0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b720ce0",
        "outputId": "e090f424-2887-4566-b8b9-e291c96dd399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "워드 임베딩 차원 : torch.Size([30000, 768])\n"
          ]
        }
      ],
      "source": [
        "word_embeddings: Tensor2D[VocabSize, EmbeddingSize] = nn.Embedding(vocab_size, 768)\n",
        "print(\"워드 임베딩 차원 :\", word_embeddings.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9be639",
      "metadata": {
        "id": "6f9be639"
      },
      "source": [
        "워드 임베딩 차원에 맞게 RNN을 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "d6cf6ed2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6cf6ed2",
        "outputId": "46c03c9b-906d-4bd2-8f21-7f18051e625d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h_0의 차원 : torch.Size([1, 1024])\n"
          ]
        }
      ],
      "source": [
        "input_size: int = word_embeddings.weight.size()[1] # RNN의 input size는 임베딩 벡터의 차원과 일치해야 합니다.\n",
        "hidden_size: int = 1024  # RNN의 hidden size\n",
        "num_layers: int = 1  # 쌓을 RNN layer의 개수\n",
        "bidirectional: bool = False  # 단방향 RNN\n",
        "\n",
        "rnn = nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional\n",
        ")\n",
        "\n",
        "# 초기 hidden state 초기화\n",
        "\n",
        "hidden_state_shape: int = (num_layers * (2 if bidirectional else 1), hidden_size)\n",
        "\n",
        "h_0: Tensor2D[Sequence, HiddenStates] = torch.zeros(hidden_state_shape)  # (num_layers * num_dirs, hidden_size)\n",
        "print(\"h_0의 차원 :\",h_0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb4f18e",
      "metadata": {
        "id": "cbb4f18e"
      },
      "source": [
        "입력 텍스트 데이터를 토크나이저를 사용하여 토큰화한 후, ids만 꺼냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "bcf10c34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcf10c34",
        "outputId": "9ca71821-a973-4970-9448-501e32ff9ba6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6227, 7125, 3302, 9046,   18])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "text: str = \"나는 학교에 간다.\"\n",
        "\n",
        "# 토큰화를 진행합니다.\n",
        "encoded = tokenizer.encode(text)\n",
        "# 토큰의 ids만 꺼냅니다.\n",
        "input_ids: List[int] = encoded.ids\n",
        "\n",
        "# 텐서화를 합니다.\n",
        "input_ids: Tensor1D[Sequence] = torch.tensor(input_ids, dtype=torch.long)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13935ca0",
      "metadata": {
        "id": "13935ca0"
      },
      "source": [
        "변환된 input_ids를 워드 임베딩으로 넣고\n",
        "워드 임베딩을 RNN의 입력으로 넣어 두 output을 얻습니다.\n",
        "\n",
        "1. `hidden_states`: 각 time step에 해당하는 hidden state들의 묶음.\n",
        "2. `h_n`: 모든 sequence를 거치고 나온 마지막 hidden state(`last hidden state`). hidden_states의 마지막과 동일."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "97d65561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d65561",
        "outputId": "635f2287-f6ba-4419-b9df-9725305f5609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "워드 임베딩 차원 :  torch.Size([5, 768])\n",
            "hidden_states 차원 :  torch.Size([5, 1024])\n",
            "h_n 차원 :  torch.Size([1, 1024])\n",
            "hidden_states의 마지막과 h_n이 같습니다.\n"
          ]
        }
      ],
      "source": [
        "input_embeds: Tensor2D[Sequence, EmbeddingSize] = word_embeddings(input_ids)\n",
        "print(\"워드 임베딩 차원 : \", input_embeds.shape)  # (vocab_size, embedding_dim)\n",
        "outputs = rnn(input_embeds, h_0)\n",
        "hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]\n",
        "h_n: Tensor2D[Layers, HiddenStates] = outputs[1]\n",
        "\n",
        "# sequence_length: input_token의 길이(length), hidden size: hidden state 차원 수, num_layers: layer 개수, num_dirs: 방향의 개수\n",
        "print(\"hidden_states 차원 : \", hidden_states.shape)  # (sequence_length, d_h)\n",
        "print(\"h_n 차원 : \", h_n.shape)  # (num_layers * num_dirs, d_h) = (1, d_h)\n",
        "\n",
        "if torch.equal(hidden_states[-1].unsqueeze(0), h_n):\n",
        "    print(\"hidden_states의 마지막과 h_n이 같습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ecc103e",
      "metadata": {
        "id": "7ecc103e"
      },
      "source": [
        "그러면 이러한 은닉 상태(hidden state)를 얻어서 어떠한 작업을 할 수 있을까요?\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 은닉 상태(hidden state)는 문장의 정보들을 압축적으로 저장합니다.</b><br>\n",
        "RNN layer를 통과하면서 문장 전체의 정보를 압축하게 되고 이러한 정보들은 hidden state에 담기게 됩니다. 이러한 hidden state는 문맥 벡터(context vector)로 사용됩니다.\n",
        "</blockquote>\n",
        "\n",
        "문맥 벡터(context vector)는 입력 문장의 정보들을 벡터상에 압축하여 저장한 것으로, 이를 통해 다양한 task를 수행할 수 있게 됩니다.\n",
        "\n",
        "여기서는 번역(translation) task를 수행하기 위해 hidden state를 사용하겠습니다.\n",
        "\n",
        "번역을 하기 위해서는 last hidden state를 다시 저희의 입력 데이터와 유사한 형태인 텍스트(토큰) id로 변환하는 layer가 필요합니다. 이를 저희는 Decoder라고 부릅니다.\n",
        "\n",
        "![image](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/Seq2SeqRNN.png)\n",
        "\n",
        "그러면 아래에서 Encoder와 Decoder를 연결하여 번역을 수행하는 모델을 구현하겠습니다.\n",
        "\n",
        "먼저 인코더를 구현하겠습니다. 위에서 구현한 rnn을 그대로 이용하여 클래스화를 진행하는 것과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "f35ff084",
      "metadata": {
        "id": "f35ff084"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Tuple\n",
        "\n",
        "class Encoder(nn.Module, ABC):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "class RNNEncoder(Encoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        bidirectional: bool,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,   # batch_first=False (default)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"입력 토큰을 임베딩하고 RNN으로 통과시켜 은닉상태 시퀀스와 마지막 은닉상태 반환\"\"\"\n",
        "        # (L, B) -> (L, B, E)\n",
        "        input_embeds = self.word_embeddings(input_ids.long())\n",
        "\n",
        "        # RNN 통과: output=(L, B, H[*num_dirs]), h_n=(num_layers[*num_dirs], B, H)\n",
        "        hidden_states, h_n = self.rnn(input_embeds)\n",
        "\n",
        "        return hidden_states, h_n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b15bc9",
      "metadata": {
        "id": "61b15bc9"
      },
      "source": [
        "다음 디코더 부분을 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "a5a3b846",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a3b846",
        "outputId": "2f99b584-ee51-4433-a9bf-d8e9d9317095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나올때 못볼기나 따름이다 나홀로집에ine い사람 어디가서 이따구\n"
          ]
        }
      ],
      "source": [
        "# 디코더 모델 또한 RNN을 사용합니다.\n",
        "class Decoder(nn.Module, ABC):\n",
        "    def __init__(self: \"Decoder\") -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, input_ids: torch.Tensor, init_hidden_state: torch.Tensor) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size,\n",
        "                 num_layers, bidirectional, start_token_id, end_token_id):\n",
        "        super().__init__()\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,     # batch_first=False\n",
        "        )\n",
        "        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        init_hidden_state: torch.Tensor,      # (num_layers*num_dirs, B, H)\n",
        "        max_len: int = 10\n",
        "    ) -> Tuple[torch.Tensor, List[int]]:\n",
        "\n",
        "        # --- h_n 차원 보정 (혹시 2D로 들어오면 3D로 복원) ---\n",
        "        h_n = init_hidden_state\n",
        "        if h_n.dim() == 2:                    # (B, H) or (H, )\n",
        "            if h_n.ndim == 2:                 # (B, H) -> (1, B, H)\n",
        "                h_n = h_n.unsqueeze(0)\n",
        "            else:\n",
        "                raise RuntimeError(\"init_hidden_state shape is invalid.\")\n",
        "\n",
        "        B = h_n.size(1)                       # 배치 크기 (여기선 1 가정)\n",
        "        device = h_n.device\n",
        "\n",
        "        logits_list: List[torch.Tensor] = []\n",
        "        # 시작 토큰 (B=1 가정)\n",
        "        input_token = torch.tensor([self.start_token_id], dtype=torch.long, device=device)\n",
        "        output_token_ids: List[int] = [input_token.item()]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if input_token.item() == self.end_token_id:\n",
        "                break\n",
        "\n",
        "            # (L=1, B=1) → 임베딩 (1,1,E)\n",
        "            emb = self.word_embeddings(input_token).unsqueeze(0)  # [1, 1, E]\n",
        "\n",
        "            # RNN 통과: output=(1,1,H), h_n=(num_layers[*dirs],1,H)\n",
        "            output, h_n = self.rnn(emb, h_n)\n",
        "\n",
        "            # 로짓 계산은 마지막 레이어의 은닉상태 사용 (shape 유지 주의!)\n",
        "            last_h = h_n[-1, 0, :]            # [H]  ← 여기서만 1D로 꺼냄\n",
        "            logit = self.fully_connected_layer(last_h)  # [V]\n",
        "            logits_list.append(logit)\n",
        "\n",
        "            # 다음 입력 토큰 (greedy)\n",
        "            input_token = torch.argmax(logit, dim=-1).unsqueeze(0)  # [1]\n",
        "            output_token_ids.append(input_token.item())\n",
        "\n",
        "        logits = torch.stack(logits_list, dim=0)  # (T, V)\n",
        "        return logits, output_token_ids\n",
        "\n",
        "start_token_id: int = tokenizer.encode(\"[CLS]\").ids[0]\n",
        "end_token_id: int = tokenizer.encode(\"[SEP]\").ids[0]\n",
        "\n",
        "vocab_size: int = 30000\n",
        "embedding_dim: int = 768\n",
        "hidden_size: int = 1024  # RNN의 hidden size\n",
        "num_layers: int = 1  # 쌓을 RNN layer의 개수\n",
        "bidirectional: bool = False  # 단방향 RNN\n",
        "\n",
        "rnn_decoder = RNNDecoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        ")\n",
        "logits, output_token_ids = rnn_decoder(h_n)\n",
        "output_texts = tokenizer.decode(output_token_ids)\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a43d683",
      "metadata": {
        "id": "2a43d683"
      },
      "source": [
        "이제 구현한 encoder와 decoder를 연결하여 seq2seq 모델을 구현해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "55b85dfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55b85dfe",
        "outputId": "6d463cd9-ee14-411e-8a06-f41d6289f036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나올때 못볼기나 따름이다 나홀로집에ine い사람 어디가서 이따구\n"
          ]
        }
      ],
      "source": [
        "class RNNSeq2Seq(nn.Module):\n",
        "    def __init__(self: \"RNNSeq2Seq\", encoder: nn.Module, decoder: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self: \"RNNSeq2Seq\", input_ids: Tensor1D[Sequence]):\n",
        "        hidden_states, context_vector = self.encoder(input_ids) # encoder에서 생성한 context_vector(h_n)을 decoder layer로 전달\n",
        "        logits, output_tokens = self.decoder(context_vector)\n",
        "\n",
        "        return logits, output_tokens\n",
        "\n",
        "seq2seq = RNNSeq2Seq(rnn_encoder, rnn_decoder)\n",
        "logits, output_tokens = seq2seq(input_ids)\n",
        "output_token_ids = logits.argmax(dim=-1)\n",
        "output_texts = tokenizer.decode(output_token_ids.tolist())\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e7487e",
      "metadata": {
        "id": "36e7487e"
      },
      "source": [
        "<blockquote>\n",
        "<b>🤔 결과값이 이상해요</b><br>\n",
        "데이터로 충분히 학습을 하지 않아서 그렇습니다. 여기서는 모델의 구조에 대해서 집중하고 추후에 모델을 학습하는 과정을 경험해보겠습니다.\n",
        "</blockquote>\n",
        "\n",
        "저희는 Sequence to Sequence(Encoder - Decoder) 구조를 이용하여 텍스트를 생성해보았습니다.\n",
        "\n",
        "Seq2Seq 구조 내에서 실제 워드 임베딩을 컨텍스트 벡터로 변환하고, 그 변환된 컨텍스트 벡터를 텍스트(토큰)으로 변환하는 과정에서 쓰인 모델은 RNN이였습니다.\n",
        "\n",
        "RNN뿐만 아니라 LSTM, 어텐션 등을 사용하여 Seq2Seq 구조를 구현할 수 있습니다.\n",
        "\n",
        "전체적인 큰 틀은 그대로 유지한 채, RNN 모듈만 바꿔주기만 하면 됩니다.\n",
        "\n",
        "그러면 이제부터 LSTM으로 다시 한번 구현해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8758c710",
      "metadata": {
        "id": "8758c710"
      },
      "source": [
        "RNN과 LSTM의 가장 큰 차이점은 LSTM에는 cell state가 추가된다는 점입니다.\n",
        "\n",
        "장기 기억을 담당하는 cell state를 통해 좀더 성능을 높일 수 있습니다.\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 Key point!</b><br>\n",
        "모델의 아키텍쳐마다 모델의 입출력이 달라집니다. 모델의 입력과 출력이 어떻게 나오는지에 대해서 이해하는 것이 중요합니다.\n",
        "</blockquote>\n",
        "\n",
        "그러면 Encoder에서 LSTM을 적용해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "bf861480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf861480",
        "outputId": "825a6ef0-c3e1-4569-dfec-cf36459d885c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_states 차원 :  torch.Size([5, 1024])\n",
            "h_n 차원 :  torch.Size([1, 1024])\n",
            "c_n 차원 :  torch.Size([1, 1024])\n"
          ]
        }
      ],
      "source": [
        "class LSTMEncoder(Encoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        bidirectional: bool,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,   # batch_first=False (기본)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor   # 기대 형태: (L, B)\n",
        "    ) -> Tuple[\n",
        "        torch.Tensor,             # hidden_states: (L, B, H * num_dirs)\n",
        "        Tuple[torch.Tensor, torch.Tensor]  # (h_n, c_n): (num_layers*num_dirs, B, H)\n",
        "    ]:\n",
        "        # (L, B) -> (L, B, E)\n",
        "        input_embeds = self.word_embeddings(input_ids)\n",
        "\n",
        "        # output: (L, B, H*num_dirs)\n",
        "        # h_n: (num_layers*num_dirs, B, H)\n",
        "        # c_n: (num_layers*num_dirs, B, H)\n",
        "        hidden_states, (h_n, c_n) = self.lstm(input_embeds)\n",
        "\n",
        "        # TODO 채움 완료\n",
        "        # hidden_states: Tensor2D[Sequence, HiddenStates]\n",
        "        # h_n: Tensor2D[Layers, HiddenStates]\n",
        "        # c_n: Tensor2D[Layers, HiddenStates]\n",
        "        return hidden_states, (h_n, c_n)\n",
        "vocab_size = 30000\n",
        "embedding_dim = 768\n",
        "hidden_size = 1024  # RNN의 hidden size\n",
        "num_layers = 1  # 쌓을 RNN layer의 개수\n",
        "bidirectional = False  # 단방향 RNN\n",
        "\n",
        "lstm_encoder = LSTMEncoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional\n",
        ")\n",
        "\n",
        "outputs = lstm_encoder(input_ids)\n",
        "hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]\n",
        "h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]\n",
        "c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]\n",
        "print(\"hidden_states 차원 : \", hidden_states.shape)  # (L, B, d_h)\n",
        "print(\"h_n 차원 : \", h_n.shape)  # (num_layers*num_dirs, B, d_h) = (1, d_h)\n",
        "print(\"c_n 차원 : \", c_n.shape)  # (num_layers*num_dirs, B, d_h) = (1, d_h)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d7ab4d",
      "metadata": {
        "id": "40d7ab4d"
      },
      "source": [
        "이번에는 LSTM을 사용하여 Decoder Layer를 구현해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "77ba9e94",
      "metadata": {
        "id": "77ba9e94",
        "outputId": "e6713b4f-accf-4c7d-e7bb-40fd04fe52e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##생에 후회 아이돌이 포르노를 올리 조디 꺙 담긴 요가만하면\n"
          ]
        }
      ],
      "source": [
        "class LSTMDecoder(Decoder):\n",
        "    def __init__(\n",
        "        self: \"LSTMDecoder\",\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        bidirectional: bool,\n",
        "        start_token_id: int,\n",
        "        end_token_id: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        # word embedding layer\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # rnn layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        # fully connected layer\n",
        "        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self: \"LSTMDecoder\",\n",
        "        init_hidden_state: Tensor2D[Layers, HiddenStates],\n",
        "        init_cell_state: Tensor2D[Layers, HiddenStates],\n",
        "        max_len: int = 10\n",
        "    ) -> Tuple[Tensor2D[MaxLength, VocabSize], List[int]]:\n",
        "        logits: List[Tensor1D[VocabSize]] = []\n",
        "        input_token: Tensor1D[Token] = torch.tensor([self.start_token_id], dtype=torch.long)\n",
        "        output_token_ids: List[int] = [input_token.item()] # tensor에서 item()을 사용하여 int로 변환합니다.\n",
        "        h_n = init_hidden_state # h_n은 encoder의 h_0와 동일한 역할을 합니다.\n",
        "        c_n = init_cell_state\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if input_token == self.end_token_id:\n",
        "                # 문장의 종료를 의미하는 special token([SEP])이 나왔다면 추론(생성)을 종료합니다.\n",
        "                break\n",
        "\n",
        "            \"\"\"직전 토큰만 입력으로 넣고 생성한 context vector는 logits에 저장합니다.\"\"\"\n",
        "            embedded: Tensor2D[Token, EmbeddingSize] = self.word_embeddings(input_token)  # 직전 입력 토큰만 사용 [1, embedding_dim]\n",
        "            outputs = self.lstm(embedded, (h_n, c_n))   # outputs: [S,B,D*H] or [B,S,D*H]\n",
        "            h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]\n",
        "            c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]\n",
        "\n",
        "            concat_h_n: Tensor1D[HiddenStates] = h_n.squeeze(0) # 여기서는 layer 갯수가 1이고, bidirectional이 False이므로 squeeze를 사용해도 무방합니다. (원래는 torch.cat으로 h_n을 합치는 작업이 필요합니다.)\n",
        "\n",
        "            \"\"\"fully connected layer를 통해 [VocabSize]의 logit을 생성합니다.\"\"\"\n",
        "            logit: Tensor1D[VocabSize] = self.fully_connected_layer(concat_h_n)\n",
        "            logits.append(logit)\n",
        "\n",
        "            \"\"\"가장 높은 점수값을 가진 토큰을 선택합니다.\"\"\"\n",
        "            input_token: Tensor1D[Token] = torch.argmax(logit, dim=-1).unsqueeze(0)\n",
        "            output_token_ids.append(input_token.item())\n",
        "\n",
        "        \"\"\"리스트의 logits를 torch의 Tensor로 변경합니다.\"\"\"\n",
        "        logits = torch.stack(logits, dim=0)  # [max_len, vocab_size]\n",
        "\n",
        "        return logits, output_token_ids\n",
        "\n",
        "\n",
        "start_token_id: int = tokenizer.encode(\"[CLS]\").ids[0]\n",
        "end_token_id: int = tokenizer.encode(\"[SEP]\").ids[0]\n",
        "\n",
        "vocab_size: int = 30000\n",
        "embedding_dim: int = 768\n",
        "hidden_size: int = 1024 # RNN의 hidden size\n",
        "num_layers: int = 1 # 쌓을 RNN layer의 개수\n",
        "bidirectional: bool = False # 단방향 RNN\n",
        "\n",
        "lstm_decoder = LSTMDecoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        ")\n",
        "\n",
        "logits, output_tokens = lstm_decoder(h_n, c_n)\n",
        "output_token_ids = logits.argmax(dim=-1)\n",
        "output_texts = tokenizer.decode(output_token_ids.tolist())\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8db0e2",
      "metadata": {
        "id": "8c8db0e2"
      },
      "source": [
        "Encoder와 Decoder를 사용하여 Seq2Seq 모델을 구현해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "fddfe053",
      "metadata": {
        "id": "fddfe053",
        "outputId": "0f1ec9f4-6f2f-4316-da3a-767c59a741b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##생에 후회 아이돌이 포르노를 올리 조디 꺙 담긴 요가만하면\n"
          ]
        }
      ],
      "source": [
        "class LSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self: \"LSTMSeq2Seq\", encoder: nn.Module, decoder: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self: \"LSTMSeq2Seq\", input_ids: Tensor1D[Sequence]):\n",
        "        hidden_states, (context_vector, cell_states) = self.encoder(input_ids) # encoder에서 생성한 context_vector(h_n)을 decoder layer로 전달\n",
        "        logits, output_tokens = self.decoder(context_vector, cell_states)\n",
        "\n",
        "        return logits, output_tokens\n",
        "\n",
        "seq2seq = LSTMSeq2Seq(lstm_encoder, lstm_decoder)\n",
        "logits, output_tokens = seq2seq(input_ids)\n",
        "output_token_ids = logits.argmax(dim=-1)\n",
        "output_texts = tokenizer.decode(output_token_ids.tolist())\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0391cd8a",
      "metadata": {
        "id": "0391cd8a"
      },
      "source": [
        "# 3. Attention Mechanism\n",
        "\n",
        "- 학습 목표\n",
        "  1. Luong Attention(Dot Attention)을 구현할 수 있다.\n",
        "  2. Attention을 이용하여 Decoder를 구현할 수 있다.\n",
        "- 학습 개념\n",
        "  1. Luong Attention\n",
        "- 진행하는 실습 요약\n",
        "  1. Luong Attention을 구현한다.\n",
        "  2. Seq2Seq 구조에 들어갈 Decoder를 구현한다.\n",
        "\n",
        "\n",
        "이번에는 Attention을 사용한 seq2seq 모델을 구현해보겠습니다.\n",
        "\n",
        "<blockquote>\n",
        "<b>🧠 Attention Mechanism</b><br>\n",
        "현재 구현할 seq2seq 모델에서의 Attention은 최근 사용하는 attention은 아닙니다. 최근의 Transformers 모델들은 Multi-Head Scaled Dot-Product Attention을 사용합니다. 해당 내용은 과제에서 다룰 예정입니다.\n",
        "</blockquote>\n",
        "\n",
        "1. 전체적인 Seq2Seq 모델의 구조는 동일합니다.\n",
        "2. Encoder에서 context vector를 얻을 때, LSTM을 사용하는 Encoder 모듈을 그대로 사용합니다.\n",
        "3. Decoder에서 output token을 생성할 때, attention mechanism을 추가합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3807ee",
      "metadata": {
        "id": "ad3807ee"
      },
      "source": [
        "그러면 우선 Dot Attention(Luong attention)을 먼저 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "70059257",
      "metadata": {
        "id": "70059257"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self: \"LuongAttention\", hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    @torch.no_grad()  # 학습 시 제거하세요\n",
        "    def forward(\n",
        "        self:\"LuongAttention\",\n",
        "        h_t: torch.Tensor,                 # (H,)\n",
        "        encoder_outputs: torch.Tensor,     # (L, H)\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"1) 쿼리(ht)를 선형변환\"\"\"\n",
        "        Wa_ht = self.W_a(h_t)              # (H,)\n",
        "\n",
        "        \"\"\"2) 점수: 각 타임스텝의 enc_hidden과 Wa_ht 내적 → (L,)\"\"\"\n",
        "        attention_score = encoder_outputs @ Wa_ht  # (L,)\n",
        "\n",
        "        \"\"\"3) 정규화(softmax) → 어텐션 가중치 (L,)\"\"\"\n",
        "        attention_weights = F.softmax(attention_score, dim=0)  # (L,)\n",
        "\n",
        "        \"\"\"4) 컨텍스트: 가중합 Σ a_t * h_enc_t → (H,)\"\"\"\n",
        "        context_vector = (attention_weights.unsqueeze(1) * encoder_outputs).sum(dim=0)  # (H,)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df83103d",
      "metadata": {
        "id": "df83103d"
      },
      "source": [
        "<blockquote>\n",
        "<b>🤔 엇 여기서도 context vector가 나오네요?</b><br>\n",
        "네 그렇습니다. 과거에는 encoder의 마지막 hidden state(h_n)을 context vector라고 불렀습니다. 하지만, attention이 나오면서 context vector는 각 디코딩 시점마다 인코더의 모든 hidden states에 대한 어텐션 가중합이라고 생각해주시면 됩니다.\n",
        "</blockquote>\n",
        "\n",
        "구현한 attention mechanism을 이용하여 Decoder layer에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "e2abb3bc",
      "metadata": {
        "id": "e2abb3bc",
        "outputId": "8404848d-230b-4192-9c09-577ce30b06ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##꾀줸ht 주구 제주 제주 제주 제주 황당하다쓰레기영화\n"
          ]
        }
      ],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self: \"AttentionDecoder\",\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        bidirectional: bool,\n",
        "        start_token_id: int,\n",
        "        end_token_id: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        # word embedding layer\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # rnn layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "\n",
        "        \"\"\"attention을 추가합니다.\"\"\"\n",
        "        self.attn = LuongAttention(hidden_size)\n",
        "        \"\"\"context vector을 입력으로 받는 trainable weights\"\"\"\n",
        "        self.W_c = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        # fully connected layer\n",
        "        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    @torch.no_grad()  # 학습 시 제거\n",
        "    def forward(\n",
        "        self:\"AttentionDecoder\",\n",
        "        init_hidden_state: Tensor1D[HiddenStates],\n",
        "        init_cell_state: Tensor1D[HiddenStates],\n",
        "        encoder_outputs: Tensor2D[Sequence, HiddenStates],\n",
        "        max_len: int = 10,\n",
        "    ):\n",
        "        logits: List[Tensor1D[VocabSize]] = []\n",
        "        input_token: Tensor1D[Token] = torch.tensor([self.start_token_id], dtype=torch.long)\n",
        "        output_token_ids: List[int] = [input_token.item()] # tensor에서 item()을 사용하여 int로 변환합니다.\n",
        "        h_n = init_hidden_state # h_n은 encoder의 h_0와 동일한 역할을 합니다.\n",
        "        c_n = init_cell_state\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if input_token == self.end_token_id:\n",
        "                # 문장의 종료를 의미하는 special token([SEP])이 나왔다면 추론(생성)을 종료합니다.\n",
        "                break\n",
        "\n",
        "            \"\"\"직전 토큰만 입력으로 넣고 생성한 context vector는 logits에 저장합니다.\"\"\"\n",
        "            embedded: Tensor2D[Token, EmbeddingSize] = self.word_embeddings(input_token)  # 직전 입력 토큰만 사용 [1, embedding_dim]\n",
        "            outputs = self.lstm(embedded, (h_n, c_n))   # outputs: [S,B,D*H] or [B,S,D*H]\n",
        "            h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]\n",
        "            c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]\n",
        "\n",
        "            concat_h_n: Tensor1D[HiddenStates] = h_n.squeeze(0) # 여기서는 layer 갯수가 1이고, bidirectional이 False이므로 squeeze를 사용해도 무방합니다. (원래는 torch.cat으로 h_n을 합치는 작업이 필요합니다.)\n",
        "\n",
        "            # 어텐션\n",
        "            context_vector, attention_weights = self.attn(concat_h_n, encoder_outputs)\n",
        "\n",
        "            \"\"\"h_n(은닉 상태)와 context_vector를 연결합니다. (Concatenate)\"\"\"\n",
        "            v_t: Tensor1D[HiddenStates * 2] = torch.cat([concat_h_n, context_vector], dim=-1)\n",
        "\n",
        "            \"\"\"v_t를 trainable weights를 통과시키고 tanh를 적용합니다.\"\"\"\n",
        "            # TODO: 직접 구현해보세요!\n",
        "            attentional_hidden_state: Tensor1D[HiddenStates] = torch.tanh(self.W_c(v_t))\n",
        "\n",
        "            \"\"\"fully connected layer를 통해 [VocabSize]의 logit을 생성합니다.\"\"\"\n",
        "            logit: Tensor1D[VocabSize] = self.fully_connected_layer(attentional_hidden_state)\n",
        "            logits.append(logit)\n",
        "\n",
        "            \"\"\"가장 높은 점수값을 가진 토큰을 선택합니다.\"\"\"\n",
        "            input_token: Tensor1D[Token] = torch.argmax(logit, dim=-1).unsqueeze(0)\n",
        "            output_token_ids.append(input_token.item())\n",
        "\n",
        "        logits = torch.stack(logits, dim=0) if logits else torch.empty(0, self.out.out_features)\n",
        "\n",
        "        return logits, output_token_ids\n",
        "\n",
        "start_token_id: int = tokenizer.encode(\"[CLS]\").ids[0]\n",
        "end_token_id: int = tokenizer.encode(\"[SEP]\").ids[0]\n",
        "\n",
        "vocab_size: int = 30000\n",
        "embedding_dim: int = 768\n",
        "hidden_size: int = 1024 # RNN의 hidden size\n",
        "num_layers: int = 1 # 쌓을 RNN layer의 개수\n",
        "bidirectional: bool = False # 단방향 RNN\n",
        "\n",
        "attention_decoder = AttentionDecoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        ")\n",
        "\n",
        "logits, output_tokens = attention_decoder(h_n, c_n, hidden_states)\n",
        "output_token_ids = logits.argmax(dim=-1)\n",
        "output_texts = tokenizer.decode(output_token_ids.tolist())\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd266f15",
      "metadata": {
        "id": "bd266f15"
      },
      "source": [
        "Decoder layer를 구현했으니 이제 Seq2Seq 모델에 적용해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "036db196",
      "metadata": {
        "id": "036db196",
        "outputId": "57ae95c0-95b3-4a7e-9a27-d4daaf224f8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##꾀줸ht 주구 제주 제주 제주 제주 황당하다쓰레기영화\n"
          ]
        }
      ],
      "source": [
        "class AttentionSeq2Seq(nn.Module):\n",
        "    def __init__(self: \"AttentionSeq2Seq\", encoder: nn.Module, decoder: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self: \"AttentionSeq2Seq\", input_ids: Tensor1D[Sequence]):\n",
        "        hidden_states, (last_hidden_state, cell_states) = self.encoder(input_ids) # encoder에서 생성한 h_n을 decoder layer로 전달\n",
        "        logits, output_tokens = self.decoder(last_hidden_state, cell_states, hidden_states)\n",
        "\n",
        "        return logits, output_tokens\n",
        "\n",
        "seq2seq = AttentionSeq2Seq(lstm_encoder, attention_decoder)\n",
        "logits, output_tokens = seq2seq(input_ids)\n",
        "output_token_ids = logits.argmax(dim=-1)\n",
        "output_texts = tokenizer.decode(output_token_ids.tolist())\n",
        "print(output_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7438c86",
      "metadata": {
        "id": "a7438c86"
      },
      "source": [
        "# 4. Huggingface 라이브러리 활용\n",
        "\n",
        "- 학습 목표\n",
        "  1. huggingface 라이브러리를 이용하여 기학습된 모델을 불러올 수 있다.\n",
        "  2. 기학습된 모델을 이용하여 추론을 할 수 있다.\n",
        "- 학습 개념\n",
        "  1. huggingface\n",
        "- 진행하는 실습 요약\n",
        "  1. HuggingFace Hub에서 한국어-영어 번역을 위해 사전학습된 모델과 토크나이저를 불러오는 코드(from_pretrained)를 완성\n",
        "  2. 불러온 토크나이저로 입력 문장을 인코딩하고, model.generate() 함수를 사용해 번역 결과를 생성하는 코드를 완성\n",
        "  3. 과제 2에서 사용한 번역 모델이 실제로 인코더와 디코더를 모두 가지고 있는지 코드로 확인\n",
        "\n",
        "huggingface는 글로벌 최대 AI 모델 오픈소스 커뮤니티입니다. 과거에는 자연어처리 모델만 있었지만, 최근에는 비전, 로봇 등 다양한 오픈소스 모델들을 지원합니다.\n",
        "\n",
        "여기서 Seq2Seq 아키텍쳐 구조에서 미리 학습한 모델을 불러와서 추론을 해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87698cfe",
      "metadata": {
        "id": "87698cfe"
      },
      "source": [
        "아래 코드를 실행하여 모델과 토크나이저를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "28f35c6a",
      "metadata": {
        "id": "28f35c6a",
        "outputId": "cc5ddb19-62a2-4c4b-8900-89cc1383a7de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d16c583",
      "metadata": {
        "id": "1d16c583"
      },
      "source": [
        "불러온 모델이 Encoder와 Decoder 모듈을 가지고 있는지 확인하는 2가지 방법이 있습니다.\n",
        "\n",
        "1. `print(model)`을 사용하여 모델의 구조를 확인합니다. 시각적으로 잘 정돈된 모델 구조를 확인할 수 있습니다.\n",
        "2. `model.named_parameters()`를 사용하여 실제 클래스를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "f818e2ea",
      "metadata": {
        "id": "f818e2ea",
        "outputId": "327e381b-a9af-4839-8270-da30db5bb59a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarianMTModel(\n",
            "  (model): MarianModel(\n",
            "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
            "    (encoder): MarianEncoder(\n",
            "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
            "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x MarianEncoderLayer(\n",
            "          (self_attn): MarianAttention(\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): SiLU()\n",
            "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder): MarianDecoder(\n",
            "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
            "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x MarianDecoderLayer(\n",
            "          (self_attn): MarianAttention(\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (activation_fn): SiLU()\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): MarianAttention(\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "bbaf7975",
      "metadata": {
        "id": "bbaf7975",
        "outputId": "b2b412df-d0d8-434f-e39e-515cfce5ea04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.shared.weight\n",
            "model.encoder.embed_positions.weight\n",
            "model.encoder.layers.0.self_attn.k_proj.weight\n",
            "model.encoder.layers.0.self_attn.k_proj.bias\n",
            "model.encoder.layers.0.self_attn.v_proj.weight\n",
            "model.encoder.layers.0.self_attn.v_proj.bias\n",
            "model.encoder.layers.0.self_attn.q_proj.weight\n",
            "model.encoder.layers.0.self_attn.q_proj.bias\n",
            "model.encoder.layers.0.self_attn.out_proj.weight\n",
            "model.encoder.layers.0.self_attn.out_proj.bias\n",
            "model.encoder.layers.0.self_attn_layer_norm.weight\n",
            "model.encoder.layers.0.self_attn_layer_norm.bias\n",
            "model.encoder.layers.0.fc1.weight\n",
            "model.encoder.layers.0.fc1.bias\n",
            "model.encoder.layers.0.fc2.weight\n",
            "model.encoder.layers.0.fc2.bias\n",
            "model.encoder.layers.0.final_layer_norm.weight\n",
            "model.encoder.layers.0.final_layer_norm.bias\n",
            "model.encoder.layers.1.self_attn.k_proj.weight\n",
            "model.encoder.layers.1.self_attn.k_proj.bias\n",
            "model.encoder.layers.1.self_attn.v_proj.weight\n",
            "model.encoder.layers.1.self_attn.v_proj.bias\n",
            "model.encoder.layers.1.self_attn.q_proj.weight\n",
            "model.encoder.layers.1.self_attn.q_proj.bias\n",
            "model.encoder.layers.1.self_attn.out_proj.weight\n",
            "model.encoder.layers.1.self_attn.out_proj.bias\n",
            "model.encoder.layers.1.self_attn_layer_norm.weight\n",
            "model.encoder.layers.1.self_attn_layer_norm.bias\n",
            "model.encoder.layers.1.fc1.weight\n",
            "model.encoder.layers.1.fc1.bias\n",
            "model.encoder.layers.1.fc2.weight\n",
            "model.encoder.layers.1.fc2.bias\n",
            "model.encoder.layers.1.final_layer_norm.weight\n",
            "model.encoder.layers.1.final_layer_norm.bias\n",
            "model.encoder.layers.2.self_attn.k_proj.weight\n",
            "model.encoder.layers.2.self_attn.k_proj.bias\n",
            "model.encoder.layers.2.self_attn.v_proj.weight\n",
            "model.encoder.layers.2.self_attn.v_proj.bias\n",
            "model.encoder.layers.2.self_attn.q_proj.weight\n",
            "model.encoder.layers.2.self_attn.q_proj.bias\n",
            "model.encoder.layers.2.self_attn.out_proj.weight\n",
            "model.encoder.layers.2.self_attn.out_proj.bias\n",
            "model.encoder.layers.2.self_attn_layer_norm.weight\n",
            "model.encoder.layers.2.self_attn_layer_norm.bias\n",
            "model.encoder.layers.2.fc1.weight\n",
            "model.encoder.layers.2.fc1.bias\n",
            "model.encoder.layers.2.fc2.weight\n",
            "model.encoder.layers.2.fc2.bias\n",
            "model.encoder.layers.2.final_layer_norm.weight\n",
            "model.encoder.layers.2.final_layer_norm.bias\n",
            "model.encoder.layers.3.self_attn.k_proj.weight\n",
            "model.encoder.layers.3.self_attn.k_proj.bias\n",
            "model.encoder.layers.3.self_attn.v_proj.weight\n",
            "model.encoder.layers.3.self_attn.v_proj.bias\n",
            "model.encoder.layers.3.self_attn.q_proj.weight\n",
            "model.encoder.layers.3.self_attn.q_proj.bias\n",
            "model.encoder.layers.3.self_attn.out_proj.weight\n",
            "model.encoder.layers.3.self_attn.out_proj.bias\n",
            "model.encoder.layers.3.self_attn_layer_norm.weight\n",
            "model.encoder.layers.3.self_attn_layer_norm.bias\n",
            "model.encoder.layers.3.fc1.weight\n",
            "model.encoder.layers.3.fc1.bias\n",
            "model.encoder.layers.3.fc2.weight\n",
            "model.encoder.layers.3.fc2.bias\n",
            "model.encoder.layers.3.final_layer_norm.weight\n",
            "model.encoder.layers.3.final_layer_norm.bias\n",
            "model.encoder.layers.4.self_attn.k_proj.weight\n",
            "model.encoder.layers.4.self_attn.k_proj.bias\n",
            "model.encoder.layers.4.self_attn.v_proj.weight\n",
            "model.encoder.layers.4.self_attn.v_proj.bias\n",
            "model.encoder.layers.4.self_attn.q_proj.weight\n",
            "model.encoder.layers.4.self_attn.q_proj.bias\n",
            "model.encoder.layers.4.self_attn.out_proj.weight\n",
            "model.encoder.layers.4.self_attn.out_proj.bias\n",
            "model.encoder.layers.4.self_attn_layer_norm.weight\n",
            "model.encoder.layers.4.self_attn_layer_norm.bias\n",
            "model.encoder.layers.4.fc1.weight\n",
            "model.encoder.layers.4.fc1.bias\n",
            "model.encoder.layers.4.fc2.weight\n",
            "model.encoder.layers.4.fc2.bias\n",
            "model.encoder.layers.4.final_layer_norm.weight\n",
            "model.encoder.layers.4.final_layer_norm.bias\n",
            "model.encoder.layers.5.self_attn.k_proj.weight\n",
            "model.encoder.layers.5.self_attn.k_proj.bias\n",
            "model.encoder.layers.5.self_attn.v_proj.weight\n",
            "model.encoder.layers.5.self_attn.v_proj.bias\n",
            "model.encoder.layers.5.self_attn.q_proj.weight\n",
            "model.encoder.layers.5.self_attn.q_proj.bias\n",
            "model.encoder.layers.5.self_attn.out_proj.weight\n",
            "model.encoder.layers.5.self_attn.out_proj.bias\n",
            "model.encoder.layers.5.self_attn_layer_norm.weight\n",
            "model.encoder.layers.5.self_attn_layer_norm.bias\n",
            "model.encoder.layers.5.fc1.weight\n",
            "model.encoder.layers.5.fc1.bias\n",
            "model.encoder.layers.5.fc2.weight\n",
            "model.encoder.layers.5.fc2.bias\n",
            "model.encoder.layers.5.final_layer_norm.weight\n",
            "model.encoder.layers.5.final_layer_norm.bias\n",
            "model.decoder.embed_positions.weight\n",
            "model.decoder.layers.0.self_attn.k_proj.weight\n",
            "model.decoder.layers.0.self_attn.k_proj.bias\n",
            "model.decoder.layers.0.self_attn.v_proj.weight\n",
            "model.decoder.layers.0.self_attn.v_proj.bias\n",
            "model.decoder.layers.0.self_attn.q_proj.weight\n",
            "model.decoder.layers.0.self_attn.q_proj.bias\n",
            "model.decoder.layers.0.self_attn.out_proj.weight\n",
            "model.decoder.layers.0.self_attn.out_proj.bias\n",
            "model.decoder.layers.0.self_attn_layer_norm.weight\n",
            "model.decoder.layers.0.self_attn_layer_norm.bias\n",
            "model.decoder.layers.0.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.0.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.0.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.0.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.0.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.0.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.0.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.0.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.0.fc1.weight\n",
            "model.decoder.layers.0.fc1.bias\n",
            "model.decoder.layers.0.fc2.weight\n",
            "model.decoder.layers.0.fc2.bias\n",
            "model.decoder.layers.0.final_layer_norm.weight\n",
            "model.decoder.layers.0.final_layer_norm.bias\n",
            "model.decoder.layers.1.self_attn.k_proj.weight\n",
            "model.decoder.layers.1.self_attn.k_proj.bias\n",
            "model.decoder.layers.1.self_attn.v_proj.weight\n",
            "model.decoder.layers.1.self_attn.v_proj.bias\n",
            "model.decoder.layers.1.self_attn.q_proj.weight\n",
            "model.decoder.layers.1.self_attn.q_proj.bias\n",
            "model.decoder.layers.1.self_attn.out_proj.weight\n",
            "model.decoder.layers.1.self_attn.out_proj.bias\n",
            "model.decoder.layers.1.self_attn_layer_norm.weight\n",
            "model.decoder.layers.1.self_attn_layer_norm.bias\n",
            "model.decoder.layers.1.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.1.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.1.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.1.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.1.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.1.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.1.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.1.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.1.fc1.weight\n",
            "model.decoder.layers.1.fc1.bias\n",
            "model.decoder.layers.1.fc2.weight\n",
            "model.decoder.layers.1.fc2.bias\n",
            "model.decoder.layers.1.final_layer_norm.weight\n",
            "model.decoder.layers.1.final_layer_norm.bias\n",
            "model.decoder.layers.2.self_attn.k_proj.weight\n",
            "model.decoder.layers.2.self_attn.k_proj.bias\n",
            "model.decoder.layers.2.self_attn.v_proj.weight\n",
            "model.decoder.layers.2.self_attn.v_proj.bias\n",
            "model.decoder.layers.2.self_attn.q_proj.weight\n",
            "model.decoder.layers.2.self_attn.q_proj.bias\n",
            "model.decoder.layers.2.self_attn.out_proj.weight\n",
            "model.decoder.layers.2.self_attn.out_proj.bias\n",
            "model.decoder.layers.2.self_attn_layer_norm.weight\n",
            "model.decoder.layers.2.self_attn_layer_norm.bias\n",
            "model.decoder.layers.2.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.2.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.2.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.2.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.2.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.2.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.2.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.2.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.2.fc1.weight\n",
            "model.decoder.layers.2.fc1.bias\n",
            "model.decoder.layers.2.fc2.weight\n",
            "model.decoder.layers.2.fc2.bias\n",
            "model.decoder.layers.2.final_layer_norm.weight\n",
            "model.decoder.layers.2.final_layer_norm.bias\n",
            "model.decoder.layers.3.self_attn.k_proj.weight\n",
            "model.decoder.layers.3.self_attn.k_proj.bias\n",
            "model.decoder.layers.3.self_attn.v_proj.weight\n",
            "model.decoder.layers.3.self_attn.v_proj.bias\n",
            "model.decoder.layers.3.self_attn.q_proj.weight\n",
            "model.decoder.layers.3.self_attn.q_proj.bias\n",
            "model.decoder.layers.3.self_attn.out_proj.weight\n",
            "model.decoder.layers.3.self_attn.out_proj.bias\n",
            "model.decoder.layers.3.self_attn_layer_norm.weight\n",
            "model.decoder.layers.3.self_attn_layer_norm.bias\n",
            "model.decoder.layers.3.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.3.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.3.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.3.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.3.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.3.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.3.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.3.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.3.fc1.weight\n",
            "model.decoder.layers.3.fc1.bias\n",
            "model.decoder.layers.3.fc2.weight\n",
            "model.decoder.layers.3.fc2.bias\n",
            "model.decoder.layers.3.final_layer_norm.weight\n",
            "model.decoder.layers.3.final_layer_norm.bias\n",
            "model.decoder.layers.4.self_attn.k_proj.weight\n",
            "model.decoder.layers.4.self_attn.k_proj.bias\n",
            "model.decoder.layers.4.self_attn.v_proj.weight\n",
            "model.decoder.layers.4.self_attn.v_proj.bias\n",
            "model.decoder.layers.4.self_attn.q_proj.weight\n",
            "model.decoder.layers.4.self_attn.q_proj.bias\n",
            "model.decoder.layers.4.self_attn.out_proj.weight\n",
            "model.decoder.layers.4.self_attn.out_proj.bias\n",
            "model.decoder.layers.4.self_attn_layer_norm.weight\n",
            "model.decoder.layers.4.self_attn_layer_norm.bias\n",
            "model.decoder.layers.4.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.4.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.4.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.4.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.4.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.4.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.4.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.4.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.4.fc1.weight\n",
            "model.decoder.layers.4.fc1.bias\n",
            "model.decoder.layers.4.fc2.weight\n",
            "model.decoder.layers.4.fc2.bias\n",
            "model.decoder.layers.4.final_layer_norm.weight\n",
            "model.decoder.layers.4.final_layer_norm.bias\n",
            "model.decoder.layers.5.self_attn.k_proj.weight\n",
            "model.decoder.layers.5.self_attn.k_proj.bias\n",
            "model.decoder.layers.5.self_attn.v_proj.weight\n",
            "model.decoder.layers.5.self_attn.v_proj.bias\n",
            "model.decoder.layers.5.self_attn.q_proj.weight\n",
            "model.decoder.layers.5.self_attn.q_proj.bias\n",
            "model.decoder.layers.5.self_attn.out_proj.weight\n",
            "model.decoder.layers.5.self_attn.out_proj.bias\n",
            "model.decoder.layers.5.self_attn_layer_norm.weight\n",
            "model.decoder.layers.5.self_attn_layer_norm.bias\n",
            "model.decoder.layers.5.encoder_attn.k_proj.weight\n",
            "model.decoder.layers.5.encoder_attn.k_proj.bias\n",
            "model.decoder.layers.5.encoder_attn.v_proj.weight\n",
            "model.decoder.layers.5.encoder_attn.v_proj.bias\n",
            "model.decoder.layers.5.encoder_attn.q_proj.weight\n",
            "model.decoder.layers.5.encoder_attn.q_proj.bias\n",
            "model.decoder.layers.5.encoder_attn.out_proj.weight\n",
            "model.decoder.layers.5.encoder_attn.out_proj.bias\n",
            "model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
            "model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
            "model.decoder.layers.5.fc1.weight\n",
            "model.decoder.layers.5.fc1.bias\n",
            "model.decoder.layers.5.fc2.weight\n",
            "model.decoder.layers.5.fc2.bias\n",
            "model.decoder.layers.5.final_layer_norm.weight\n",
            "model.decoder.layers.5.final_layer_norm.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167a381f",
      "metadata": {
        "id": "167a381f"
      },
      "source": [
        "이미 학습된 모델을 통해 추론을 진행합니다.\n",
        "위의 실습에서 추론했던 것과는 다르게 학습된 모델이므로 성능이 더 높게 나타납니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "7f0ec813",
      "metadata": {
        "id": "7f0ec813",
        "outputId": "0c214702-f4c9-4067-de7e-7e4969b0c064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC: 나는 학교에 간다.\n",
            "MT : I'm going to school.\n"
          ]
        }
      ],
      "source": [
        "text = \"나는 학교에 간다.\"\n",
        "\"\"\"여기서는 batch로 입력을 처리하여 차원이 [seq_len]이 아닌 [batch_size, seq_len]입니다. 여기서는 입력이 한개이므로 [1, seq_len]입니다.\"\"\"\n",
        "encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **encoded,\n",
        "    max_new_tokens=64,\n",
        ")\n",
        "\n",
        "translation = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
        "print(\"SRC:\", text)\n",
        "print(\"MT :\", translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0383cf9c",
      "metadata": {
        "id": "0383cf9c"
      },
      "source": [
        "# 5. 아키텍처별 모델 다뤄보기(Encoder model, Decoder model)\n",
        "\n",
        "- 학습 목표\n",
        "  1. huggingface 라이브러리를 이용하여 다양한 모델 구조의 모델을 다룰 수 있다.\n",
        "- 학습 개념\n",
        "  1. huggingface\n",
        "- 학습 내용\n",
        "  1. 문맥을 양방향으로 이해하는 데 강점이 있는 BERT 모델을 사용하여 문장의 빈칸([MASK])에 가장 적절한 단어를 추론\n",
        "  2. 이전 텍스트를 바탕으로 다음 텍스트를 생성하는 데 특화된 GPT-2 모델을 사용하여 이야기의 뒷부분을 창작\n",
        "\n",
        "지금까지는 Seq2Seq(Encoder - Decoder) 모델 구조를 다뤘습니다. 하지만, 현재 가장 많이 사용되는 모델은 Only Decoder 모델입니다.\n",
        "\n",
        "1. Only Encoder 모델 : BERT 같은 모델. RAG등 문서 검색에 주로 사용\n",
        "2. Only Decoder 모델 : Chat-GPT 같은 모델. 대화, 번역, 챗봇 등 현재 가장 많이 사용\n",
        "3. Encoder - Decoder 모델 : 최근에는 잘 사용하지 않음\n",
        "\n",
        "그러면 Only Encoder 모델과 Only Decoder 모델을 이용해 모델 추론을 해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8039a153",
      "metadata": {
        "id": "8039a153"
      },
      "source": [
        "Encoder의 대표 모델인 BERT 모델을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "3ddc1fad",
      "metadata": {
        "id": "3ddc1fad",
        "outputId": "9e53839a-50b6-45be-fda8-897132cd1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4665108b",
      "metadata": {
        "id": "4665108b"
      },
      "source": [
        "BERT 모델을 이용하여 빈칸 맞추기(Masked Language Modeling)를 추론해봅니다.\n",
        "\n",
        "예를 들어, I [MASK] to school. 이라는 문장에서 [MASK]에 들어갈 단어를 맞춘다고 하면 I go to school. 이 문장이 정답이 됩니다.\n",
        "\n",
        "하지만, I went to school도 정답이 될 수 있습니다.\n",
        "\n",
        "이처럼 [MASK]에 들어갈 단어는 여러가지가 될 수 있고, 모델의 학습에 따라 어떤 단어가 [MASK]에 들어갈지 결정됩니다.\n",
        "\n",
        "이러한 특성을 이용하여 BERT 모델을 이용하여 빈칸 맞추기(`[MASK]`)를 추론해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "4ce98fb7",
      "metadata": {
        "id": "4ce98fb7",
        "outputId": "503a22fe-8442-4419-aa5e-cdda0556b1a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 문장: I [MASK] to school.\n",
            "BERT가 예측한 문장들:\n",
            "1순위: I went to school.\n",
            "2순위: I go to school.\n",
            "3순위: I walked to school.\n",
            "4순위: I ran to school.\n",
            "5순위: I got to school.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from typing import List, Tuple\n",
        "\n",
        "# 4. 우리가 맞출 문장 만들기. tokenizer.mask_token = \"[MASK]\" 이 부분이 빈칸이 됨\n",
        "sentence = f\"I {tokenizer.mask_token} to school.\"\n",
        "\n",
        "top_k = 5  # 상위 5개 후보\n",
        "\n",
        "# 5. 문장을 숫자로 변환\n",
        "encoded = tokenizer(sentence, return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "# 6. 입력 토큰 ID\n",
        "input_ids = encoded.input_ids\n",
        "\n",
        "# 7. [MASK] 토큰 ID\n",
        "mask_token_id = tokenizer.mask_token_id\n",
        "\n",
        "# 8. [MASK] 위치 찾기 (배치, 토큰위치)\n",
        "mask_positions = (input_ids == mask_token_id).nonzero(as_tuple=False)\n",
        "assert mask_positions.numel() > 0, \"문장에 [MASK] 토큰이 없습니다.\"\n",
        "\n",
        "# 9. 모델 추론\n",
        "outputs = model(**encoded)\n",
        "\n",
        "# 10. 위치별 vocab 로짓\n",
        "logits = outputs.logits.squeeze(0)  # (seq_len, vocab_size)\n",
        "\n",
        "# 11. 모든 [MASK] 위치에 대해 예측\n",
        "all_token_candidates: List[List[Tuple[str, float]]] = []\n",
        "for _, pos in mask_positions:\n",
        "    pos = pos.item()\n",
        "    logits_at_pos = logits[pos]                      # (vocab_size,)\n",
        "    probs = torch.softmax(logits_at_pos, dim=-1)     # 확률화\n",
        "    topk = torch.topk(probs, k=top_k)                # 상위 K\n",
        "\n",
        "    ids = topk.indices.tolist()\n",
        "    scores = topk.values.tolist()\n",
        "    tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in ids]\n",
        "\n",
        "    candidates = list(zip(tokens, scores))\n",
        "    all_token_candidates.append(candidates)\n",
        "\n",
        "# 12. 복원 문장 담을 리스트\n",
        "restored_sentences: List[str] = []\n",
        "\n",
        "# 13. 첫 번째 [MASK] 위치 후보들만 사용 (여러 MASK면 필요에 따라 확장)\n",
        "token_candidates: List[Tuple[str, float]] = all_token_candidates[0]\n",
        "\n",
        "# 14. 후보 단어를 하나씩 [MASK] 자리에 넣어 문장 복원\n",
        "for tok, _ in token_candidates:\n",
        "    new_ids = input_ids.clone()\n",
        "    tok_id = tokenizer.convert_tokens_to_ids(tok)\n",
        "    new_ids[0, mask_positions[0, 1]] = tok_id\n",
        "    text = tokenizer.decode(new_ids[0], skip_special_tokens=True)\n",
        "    restored_sentences.append(text.strip())\n",
        "\n",
        "# 15. 결과 출력\n",
        "print(\"원본 문장:\", sentence)\n",
        "print(\"BERT가 예측한 문장들:\")\n",
        "for idx, sent in enumerate(restored_sentences, start=1):\n",
        "    print(f\"{idx}순위: {sent}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9e04c7",
      "metadata": {
        "id": "be9e04c7"
      },
      "source": [
        "Only Decoder 모델의 대표인 GPT 모델을 이용하여 추론을 해보겠습니다.\n",
        "\n",
        "GPT-2 모델을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "6a49ab04",
      "metadata": {
        "id": "6a49ab04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feb32c0",
      "metadata": {
        "id": "0feb32c0"
      },
      "source": [
        "GPT-2 모델은 입력으로 토큰화된 텍스트를 받고, 그 뒤에 올 단어들을 예측(Next token Prediction)하는 것이 목표입니다.\n",
        "\n",
        "아래 코드를 이용하여 스토리(입력 텍스트)의 뒷 내용을 생성해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "33182f00",
      "metadata": {
        "id": "33182f00",
        "outputId": "19cd5d1b-3d1e-4fff-8686-4656a782ec2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village, a curious child found a mysterious key. The child was a boy named Kiyoshi. He was a boy who had been born with a strange, mysterious, and mysterious voice. He was a boy who had been born with a strange, mysterious, and mysterious voice. He was a boy who had been born with a strange, mysterious, and mysterious voice.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time in a small village, a curious child found a mysterious key.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "    )\n",
        "\n",
        "output_tokens = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
        "print(output_tokens)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "c10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}