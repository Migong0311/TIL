
# ğŸ§  **Day 04 â€” PyTorch ê¸°ë°˜ MLP ì‹¤ìŠµ (ì‹œí—˜ ëŒ€ë¹„ìš© ì •ë¦¬)**

---

## ğŸ“˜ 1ï¸âƒ£ MLP (Multi-Layer Perceptron) ê¸°ë³¸ ê°œë…

| í•­ëª©      | ì„¤ëª…                                                                           |
| --------- | ------------------------------------------------------------------------------ |
| ì •ì˜      | ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ(1ê°œ ì´ìƒ) â†’ ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ **ê¸°ë³¸ ì‹ ê²½ë§ êµ¬ì¡°**             |
| ì£¼ìš” ì—°ì‚° | `Linear â†’ ReLU â†’ Dropout`                                                      |
| ì—­í•       | ë¹„ì„ í˜• í•¨ìˆ˜(í™œì„±í™” í•¨ìˆ˜)ë¡œ ë³µì¡í•œ ê´€ê³„ë¥¼ í•™ìŠµ                                  |
| íŠ¹ì§•      | ì™„ì „ì—°ê²°ì¸µ(fully connected), ë°ì´í„°ì˜ êµ¬ì¡° ì •ë³´(ê³µê°„ì /ì‹œê³„ì—´)ëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ |

### âœ… ê¸°ë³¸ êµ¬ì¡° ì˜ˆì‹œ

```python
class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dims=(128, 64), dropout=0.2):
        super().__init__()
        h1, h2 = hidden_dims
        self.net = nn.Sequential(
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h2, num_classes)
        )
    def forward(self, x):
        return self.net(x)
```

> ğŸ”¹ `nn.Linear` : ì…ë ¥ â†’ ì¶œë ¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³± + í¸í–¥(bias)
> ğŸ”¹ `nn.ReLU` : ë¹„ì„ í˜•ì„± ë¶€ì—¬
> ğŸ”¹ `nn.Dropout(p)` : í•™ìŠµ ì‹œ p ë¹„ìœ¨ë§Œí¼ ë‰´ëŸ°ì„ ë¹„í™œì„±í™” (ê³¼ì í•© ë°©ì§€)

---

## âš™ï¸ 2ï¸âƒ£ `model(x)`ì˜ ë‚´ë¶€ ë™ì‘

| í˜¸ì¶œ                                           | ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ë©”ì„œë“œ               |
| ---------------------------------------------- | ---------------------------------------- |
| `model(x)`                                     | `model.__call__(x)` â†’ `model.forward(x)` |
| ì¦‰, ê´„í˜¸ `()`ëŠ” **forward propagation**ì„ ì˜ë¯¸ |                                          |

> âœ… forward: ì…ë ¥ â†’ ì˜ˆì¸¡(logits)
> âœ… backward: ì†ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì—­ì „íŒŒ(gradient ê³„ì‚°)

---

## ğŸ§© 3ï¸âƒ£ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ êµ¬ì¡°

### ğŸ”¹ (1) í•™ìŠµ ë£¨í”„ â€” `train_one_epoch()`

```python
def train_one_epoch(model, loader, optimizer, device):
    model.train()
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        logits = model(xb)
        loss = F.cross_entropy(logits, yb.squeeze(1).long())
        loss.backward()
        optimizer.step()
```

| ë‹¨ê³„                    | ì˜ë¯¸                        |
| ----------------------- | --------------------------- |
| `optimizer.zero_grad()` | ì´ì „ stepì˜ gradient ì´ˆê¸°í™” |
| `loss.backward()`       | ì—­ì „íŒŒ(gradient ê³„ì‚°)       |
| `optimizer.step()`      | íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸           |

> ğŸ”¹ ì‹œí—˜í¬ì¸íŠ¸: `zero_grad()` ìƒëµ ì‹œ â†’ ì´ì „ batchì˜ gradientê°€ ëˆ„ì ë˜ì–´ ì˜ëª»ëœ ì—…ë°ì´íŠ¸ ë°œìƒ

---

### ğŸ”¹ (2) í‰ê°€ ë£¨í”„ â€” `evaluate()`

```python
def evaluate(model, loader, device):
    model.eval()
    with torch.no_grad():
        for xb, yb in loader:
            logits = model(xb)
            preds = logits.argmax(dim=1)
```

| í•­ëª©              | ì„¤ëª…                                     |
| ----------------- | ---------------------------------------- |
| `model.eval()`    | Dropout ë¹„í™œì„±í™”, BNì˜ ì´ë™í‰ê·  ì‚¬ìš©     |
| `torch.no_grad()` | gradient ê³„ì‚° ë¹„í™œì„±í™”(ì†ë„ â†‘, ë©”ëª¨ë¦¬ â†“) |
| `argmax(dim=1)`   | ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ                  |

> ğŸ”¹ ì‹œí—˜í¬ì¸íŠ¸: `train()` vs `eval()`ì˜ Dropout ì°¨ì´ ë°˜ë“œì‹œ ìˆ™ì§€

---

## ğŸ§  4ï¸âƒ£ `train()` vs `eval()` ë¹„êµ ìš”ì•½

| ëª¨ë“œ        | Dropout                  | BatchNorm             | ì‚¬ìš© ì‹œì  |
| ----------- | ------------------------ | --------------------- | --------- |
| **train()** | í™œì„±í™” (ë¬´ì‘ìœ„ ë¹„í™œì„±í™”) | ë°°ì¹˜ë³„ í‰ê· /ë¶„ì‚° ì‚¬ìš© | í•™ìŠµ ë‹¨ê³„ |
| **eval()**  | ë¹„í™œì„±í™”                 | ì €ì¥ëœ ì´ë™ í‰ê·  ì‚¬ìš© | ì¶”ë¡  ë‹¨ê³„ |

> âœ… ì˜¤ë‹µì£¼ì˜: `model.eval()`ì€ gradientë¥¼ ë„ì§€ ì•ŠìŠµë‹ˆë‹¤.
> (â†’ ë°˜ë“œì‹œ `torch.no_grad()`ì™€ í•¨ê»˜ ì¨ì•¼ í•¨)

---

## ğŸ§© 5ï¸âƒ£ ê³¼ì í•©(Overfitting) ë°©ì§€ ì „ëµ

| ì „ëµ                    | ì„¤ëª…                                               |
| ----------------------- | -------------------------------------------------- |
| Dropout                 | ì¼ë¶€ ë‰´ëŸ° ë¬´ì‘ìœ„ ë¹„í™œì„±í™”                          |
| L2 ì •ê·œí™”               | ê°€ì¤‘ì¹˜ í¬ê¸°ì— íŒ¨ë„í‹° ë¶€ì—¬ (`weight_decay`)         |
| Early Stopping          | ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ë©´ í•™ìŠµ ì¤‘ë‹¨                     |
| Learning Rate Scheduler | í•™ìŠµë¥  ì ì§„ì  ê°ì†Œ (`StepLR`, `ReduceLROnPlateau`) |
| Data Augmentation       | ì…ë ¥ ë°ì´í„° ë‹¤ì–‘í™”                                 |

> ğŸ”¹ ì‹œí—˜í¬ì¸íŠ¸: Dropoutì€ í•™ìŠµ ì‹œë§Œ ì ìš©ë˜ê³ , í‰ê°€ ì‹œì—” ë¹„í™œì„±í™”

---

## ğŸ§® 6ï¸âƒ£ í•™ìŠµ í›„ í‰ê°€ (sklearn í™œìš©)

```python
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(targets, preds))
print(classification_report(targets, preds))
```

| í•¨ìˆ˜                      | ì„¤ëª…                               |
| ------------------------- | ---------------------------------- |
| `confusion_matrix()`      | í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì •ì˜¤í‘œ               |
| `classification_report()` | Precision / Recall / F1-score ì¶œë ¥ |

> ğŸ”¹ ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ í‰ê°€ ì‹œ sklearnê³¼ í•¨ê»˜ ìì£¼ ì‚¬ìš©

---

## âš¡ 7ï¸âƒ£ ì „ì²´ í•™ìŠµ íë¦„ ìš”ì•½

| ë‹¨ê³„              | ì„¤ëª…                              | ì£¼ìš” í•¨ìˆ˜                             |
| ----------------- | --------------------------------- | ------------------------------------- |
| â‘  ë°ì´í„° ì „ì²˜ë¦¬   | í‘œì¤€í™” í›„ Tensor ë³€í™˜             | `StandardScaler`, `TensorDataset`     |
| â‘¡ DataLoader êµ¬ì„± | ë¯¸ë‹ˆë°°ì¹˜ ì œê³µ                     | `DataLoader(batch_size)`              |
| â‘¢ ëª¨ë¸ ì •ì˜       | ì…ë ¥-ì€ë‹‰-ì¶œë ¥ì¸µ êµ¬ì„±             | `nn.Linear`, `nn.ReLU`, `nn.Dropout`  |
| â‘£ í•™ìŠµ ë£¨í”„       | ìˆœì „íŒŒ â†’ ì†ì‹¤ â†’ ì—­ì „íŒŒ â†’ ì—…ë°ì´íŠ¸ | `loss.backward()`, `optimizer.step()` |
| â‘¤ í‰ê°€ ë£¨í”„       | ì¶”ë¡  ë° ì •í™•ë„ ê³„ì‚°               | `model.eval()`, `torch.no_grad()`     |
| â‘¥ ì„±ëŠ¥ í‰ê°€       | ì •í™•ë„, F1-score                  | `classification_report()`             |

---

## ğŸ§¾ 8ï¸âƒ£ ì¶”ê°€ ê°œë… ì •ë¦¬ (ì‹œí—˜ ìì£¼ ì¶œì œ)

| ê°œë…                    | ì„¤ëª…                                                       |
| ----------------------- | ---------------------------------------------------------- |
| **Batch size**          | í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ê°œìˆ˜                    |
| **Epoch**               | ì „ì²´ ë°ì´í„°ì…‹ì´ í•œ ë²ˆ í•™ìŠµë˜ëŠ” ë‹¨ìœ„                        |
| **Optimizer ì¢…ë¥˜**      | `SGD`, `Adam`, `RMSProp` ë“± (Adamì´ ê°€ì¥ ìì£¼ ì‚¬ìš©)        |
| **Loss Function**       | ë¶„ë¥˜ëŠ” `CrossEntropyLoss`, íšŒê·€ëŠ” `MSELoss`                |
| **Gradient Descent**    | ì†ì‹¤ ìµœì†Œí™”ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ê°±ì‹  ì•Œê³ ë¦¬ì¦˜                  |
| **Activation Function** | ë¹„ì„ í˜•ì„± ë¶€ì—¬ (`ReLU`, `Sigmoid`, `Tanh`)                  |
| **Sequential ëª¨ë¸**     | ìˆœì°¨ì ìœ¼ë¡œ layerë¥¼ ìŒ“ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ êµ¬ì¡° (`nn.Sequential`) |

---

## âœ… í•µì‹¬ ìš”ì•½ (ì‹œí—˜ ëŒ€ë¹„ í¬ì¸íŠ¸)

1. `model(x)`ì€ ë‚´ë¶€ì ìœ¼ë¡œ `__call__()` â†’ `forward()` ì‹¤í–‰
2. `train()` ëª¨ë“œì—ì„œëŠ” Dropout í™œì„±í™” / `eval()` ëª¨ë“œì—ì„œëŠ” ë¹„í™œì„±í™”
3. `zero_grad()`, `backward()`, `step()`ì˜ ìˆœì„œ ì¤‘ìš”
4. `torch.no_grad()`ëŠ” í‰ê°€ ì‹œ gradient ë¹„í™œì„±í™”ìš©
5. Overfitting ë°©ì§€ ë°©ë²• 3ê°€ì§€ ì´ìƒ ì„œìˆ  ê°€ëŠ¥í•´ì•¼ í•¨
6. í•™ìŠµ í›„ sklearnì˜ `confusion_matrix`, `classification_report`ë¡œ í‰ê°€
7. MLPëŠ” CNN/RNNë³´ë‹¤ ë‹¨ìˆœí•˜ì§€ë§Œ êµ¬ì¡° ì´í•´ì˜ ê¸°ë³¸

---

> ğŸ’¡ **í•œì¤„ ìš”ì•½:**
> â€œMLPëŠ” PyTorch ì‹ ê²½ë§ì˜ ê¸°ë³¸ êµ¬ì¡°ë¡œ, Linear + ReLU + Dropoutìœ¼ë¡œ êµ¬ì„±ë˜ë©°
> í•™ìŠµ ë£¨í”„ëŠ” zero_grad â†’ forward â†’ loss â†’ backward â†’ step ìˆœìœ¼ë¡œ ë°˜ë³µëœë‹¤.â€

